\section{Function class of asymmetric inner product relations}\label{sec:asymmetric_relations}

In the previous section we considered symmetric inner product relations where the encoder of the first object is the same as the encoder of the second object. When the underlying relation being modeled is a symmetric `similarity' relation, this is a useful inductive bias. However, in general, relations between objects can be asymmetric. For example, order relations are asymmetric (in fact, anti-symmetric). Such relations cannot be captured by symmetric inner products. In this section, we consider modeling a general (asymmetric) relation $r: \calX \times \calX$ as the inner product of two different neural network encodings of a pair of objects,
\begin{equation}\label{eq:asymmetric_iprod_relation}
    r(x, y) = \iprod{\phi(x)}{\psi(y)},
\end{equation}
where $\phi, \psi: \calX \to \reals^d$ are two neural networks.

In this section, we show that inner products of universal approximators (e.g., when $\phi, \psi$ are multi-layer perceptrons) can approximate any continuous function on $\calX \times \calX$.

We begin with the following simple lemma which states when the object space $\calX$ is finite, any relation function can be represented as the inner product between two encodings.

\begin{lemma}\label{lemma:finite_space_rel}
    Suppose $\calX$ is a finite space. Let $r: \calX \times \calX \to \reals$ be any relation function. Then, there exists $d \in \bbN$ and $\phi, \psi : \calX \to \reals^{d}$ such that,
    \begin{equation*}
        r(x, y) = \iprod{\phi(x)}{\psi(y)}, \ \forall x, y \in \calX.
    \end{equation*}
\end{lemma}

\begin{proof}
    \hphantom{~}

    Let $x_1, \ldots, x_m$ be an enumerate of $\calX$ where $m = \abs{\calX}$. Let $R \in \reals^{m \times m}$ such that $R_{ij} = r(x_i, x_j)$. There exists many decompositions of the matrix $R$ which would induce valid encodings $\phi, \psi$. One example is rank decomposition. Let $d = \mathrm{rank}(R)$. Then, there exists matrices $P \in \reals^{m \times d}, Q \in \reals^{d \times m}$ such that $R = P Q$. Let $\phi, \psi: \calX \to \reals^{d}$ be defined by
    \begin{equation}
        \phi(x_i) = P_{i, \cdot}, \ \psi(x_i) = Q_{\cdot, i}, \ \forall i \in [m].
    \end{equation}

    Then, $r(x, y) = \iprod{\phi(x)}{\psi(y)}$ for all $x, y \in \calX$.
\end{proof}

With this result, we can show that any continuous function on $\calX \times \calX$ can be approximated by inner products of neural networks (or other universal approximators). The argument proceeds by first quantizing the space $\calX$ and using~\Cref{lemma:finite_space_rel} to represent the relation function as an inner product of encodings. Then, a similar argument to~\Cref{theorem:symmetric_inner_prod_rels_func_class} shows the result.

\begin{theorem}\label{theoorem:asymemtric_inner_prod_rel_func_class}
    Suppose $\calX$ is a compact subset of euclidean space. Consider the relation model,
    \begin{equation*}
        \hat{r}(x, y) := \iprod{\phi_\theta(x)}{\psi_\theta(y)},
    \end{equation*}
    \noindent where $\phi_\theta, \psi_\theta: \calX \to \reals^d$ are multi-layer perceptrons with parameters $\theta$. Then, for any continuous relation function $r: \calX \times \calX \to \reals$ there exists multi-layer perceptrons with parameters $\theta$ such that $\hat{r}$ approximates $r$ uniformly over $(x,y) \in \calX \times \calX$. Formally, for any $\epsilon > 0$, there exists neural networks with parameters $\theta$ such that,
    \begin{equation*}
        \sup_{x, y \in \calX} \abs{r(x,y) - \hat{r}(x,y)} < \epsilon.
    \end{equation*}
\end{theorem}

\begin{proof}
    \hphantom{~}

    Let $\calN_\delta(\calX)$ be a $\delta$-net of $\calX$. That is, a finite subset of $\calX$ such that $\max_{x \in \calX} \min_{y \in \calN_\delta(\calX)} \norm{x - y} < \delta$. Let $q$ be a projection from $\calX$ on to the $\delta$-net. That is, $q(x) \in \argmin_{y \in \calN_\delta(\calX)} \norm{x - y}$. By the continuity of $r$ and compactness of $\calX$, let $\delta > 0$ be small enough such that,
    \begin{equation*}
        \sup_{x, y \in \calX} \abs{r(x,y) - r(q(x), q(y))} < \varepsilon / 2.
    \end{equation*}
    Since $\calN_\delta(\calX)$ is a finite set, by~\Cref{lemma:finite_space_rel}, there exists $d \in \bbN$ and $\phi, \psi: \calN_\delta(\calX) \to \reals^d$ such that $r(x, y) = \iprod{\phi(x)}{\psi(y)}, \ \forall x,y \in \calN_\delta(\calX)$. Thus, $r(q(x), q(y)) = \iprod{(\phi \circ q)(x)}{(\psi \circ q)(y)}, \ \forall x,y \in \calX$.  Now, we would like to approximate $\phi \circ q$ with $\phi_\theta$ and $\psi \circ q$ with $\psi_\theta$.

    For any $x, y \in \calX$, we have,
    \begin{equation*}
        \begin{split}
            &\abs{\iprod{(\phi \circ q)(x)}{(\psi \circ q)(y)} - \iprod{\phi_\theta(x)}{\psi_\theta(y)}} \\
            &= \abs{\sum_{i=1}^{d} \paren{((\phi \circ q)(x))_i ((\psi \circ q)(y))_i - (\phi_\theta(x))_i (\psi_\theta(y))_i}} \\
            &\leq \sum_{i=1}^{d} \abs{((\phi \circ q)(x))_i ((\psi \circ q)(y))_i - (\phi_\theta(x))_i (\psi_\theta(y))_i} \\
        &\leq \sum_{i=1}^d \paren{ \abs{(\phi_\theta(x))_i} \abs{((\psi \circ q)(y))_i - (\psi_\theta(y))_i} + \abs{(\psi_\theta(y))_i} \abs{((\phi \circ q)(x))_i - (\phi_\theta(x))_i}} \\
    \end{split}
\end{equation*}

Note that $\phi \circ q$ and $\psi \circ q$ are continuous (in fact, piecewise constant) functions on a compact euclidean space $\calX$. By the universal approximation of multi-layer perceptrons, there exists $\theta$ such that
\begin{equation*}
    \sup_{x, y \in \calX} \abs{\iprod{(\phi \circ q)(x)}{(\psi \circ q)(y)} - \iprod{\phi_\theta(x)}{\psi_\theta(y)}} < \frac{\varepsilon}{2}
\end{equation*}

Hence, we have,
\begin{equation*}
    \begin{split}
        &\sup_{x, y \in \calX} \abs{r(x, y) - \iprod{\phi_\theta(x)}{\psi_\theta(y)}} \\
        &\leq \sup_{x, y \in \calX} \abs{r(x, y) - r(q(x), q(y))} + \sup_{x, y \in \calX} \abs{r(q(x), q(y)) - \iprod{\phi_\theta(x)}{\psi_\theta(y)}} \\
        &\leq  \sup_{x, y \in \calX} \abs{r(x, y) - r(q(x), q(y))} + \sup_{x, y \in \calX} \abs{\iprod{(\phi \circ q)(x)}{(\psi \circ q)(y)} - \iprod{\phi_\theta(x)}{\psi_\theta(y)}}  \\
        &< \frac{\varepsilon}{2} + \frac{\varepsilon}{2} = \varepsilon.
    \end{split}
\end{equation*}


\end{proof}