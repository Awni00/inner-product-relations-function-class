\section{Function class of asymmetric inner product relations}\label{sec:asymmetric_relations}

In the previous section we considered symmetric inner product relations where the encoder of the first object is the same as the encoder of the second object. When the underlying relation being modeled is a symmetric `similarity' relation, this is a useful inductive bias. However, in general, relations between objects can be asymmetric. For example, order relations are asymmetric (in fact, anti-symmetric). Such relations cannot be captured by symmetric inner products. In this section, we consider modeling a general (asymmetric) relation $r: \calX \times \calX$ as the inner product of two different neural network encodings of a pair of objects,
\begin{equation}\label{eq:asymmetric_iprod_relation}
    r(x, y) = \iprod{\phi(x)}{\psi(y)},
\end{equation}
where $\phi, \psi: \calX \to \reals^d$ are two neural networks.

In this section, we show that inner products of universal approximators (e.g., when $\phi, \psi$ are multi-layer perceptrons) can approximate any continuous function on $\calX \times \calX$.

We begin with the following simple lemma which states when the object space $\calX$ is finite, any relation function can be represented as the inner product between two encodings.

\begin{lemma}\label{lemma:finite_space_rel}
    Suppose $\calX$ is a finite space. Let $r: \calX \times \calX \to \reals$ be any relation function. Then, there exists $d \leq \abs{\calX}$ and $\phi, \psi: \calX \to \reals^{d}$ such that,
    \begin{equation*}
        r(x, y) = \iprod{\phi(x)}{\psi(y)}, \ \forall x, y \in \calX.
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $x_1, \ldots, x_n$ be an enumeration of $\calX$ where $m = \abs{\calX}$. Let $R \in \reals^{n \times n}$ such that $R_{ij} = r(x_i, x_j)$. There exists many decompositions of the matrix $R$ which would induce valid encodings $\phi, \psi$. One example is rank decomposition. Let $d = \mathrm{rank}(R)$. Then, there exists matrices $P, Q \in \reals^{d \times n}$ such that $R = P^\top Q$. Let $\phi, \psi: \calX \to \reals^{d}$ be defined by
    \begin{equation}
        \phi(x_i) = P_{i, \cdot}, \ \psi(x_i) = Q_{\cdot, i}, \ \forall i \in [m].
    \end{equation}

    Then, $r(x, y) = \iprod{\phi(x)}{\psi(y)}$ for all $x, y \in \calX$.
\end{proof}

Note that if each $x \in \calX$ is a one-hot vector in $\reals^{\abs{\calX}}$, then the result above holds with linear maps $\phi, \psi$. Although very simple, this result has direct implications to domains such as language modeling where $\calX$ is a discrete set of tokens, and hence finite. In such cases,~\Cref{lemma:finite_space_rel} tells us that any relation function can be approximated by inner products of feature maps (i.e., of the form present in the attention mechanisms of Transformers). Moreover, in the case of language, there may be a low-rank structure (e.g., depending on syntax, semantics, etc.) enabling a more modest dimension of the feature maps, $d \ll \abs{\calX}$.

Next, we proceed to show that arbitrary continuous relation functions can be approximated by inner products of two different neural networks. Our strategy will be to first quantize the space $\calX$ then apply the construction above for the finite case.

\begin{theorem}\label{theorem:asymemtric_inner_prod_rel_func_class}
    Suppose the relation function $r: \calX \times \calX \to \reals$ is continuous. In particular, for any $\epsilon > 0$, there exists $\delta(\epsilon) > 0$ such that for any $x, y, \tilde{x}, \tilde{y}$ satisfying $\norm{x - \tilde{x}} \leq \delta$, $\norm{y - \tilde{y}} \leq \delta$, we have $\abs{r(x, y) - r(\tilde{x}, \tilde{y})} \leq \epsilon$. Then, for any approximation error $\epsilon > 0$ there exists multi-layer perceptrons $\phi, \psi$ such that
    \begin{equation*}
        \abs{r(x,y) - \iprod{\phi(x)}{\psi(y)}} \leq \epsilon, \quad \text{Lebesgue-almost everywhere.}
    \end{equation*}
    Moreover, $\phi, \psi$ can be constructed such that $\phi = L_\phi \circ \eta$, $\psi = L_\psi \circ \eta$ where $\eta$ is a shared 2-layer MLP with $N = \calO(\delta^{- 2 \,\dim(\calX)})$ neurons and $L_\phi, L_\psi$ are linear projections onto $n$-dimensional space, with $n = \calO(\delta^{- \dim(\calX)})$ and $\delta = \delta(\epsilon)$.
\end{theorem}
\begin{proof}
    Let $x_1, \ldots, x_n \in \calX$ be a set of points in $\calX$. Define the Voronoi partition by
    \[V^{(i)} = \sset{x \in \calX \,|\, \norm{x - x_i} \leq \norm{x - x_j} \ \forall j \neq i}.\]
    Let $x_1, \ldots, x_n$ be uniformly distributed in $\calX$ and $n = \calO(\delta(\epsilon)^{- \dim(\calX)})$ so that the maximal diameter of the sets $V^{(1)}, \ldots, V^{(n)}$ is bounded by $\delta(\epsilon)$, $\max_{i \in [n]} \mathrm{diam}(V^{(i)}) \leq \delta$. Let $q: \calX \to \sset{x_1, \ldots, x_n}$ be the quantizer which maps each $x$ to the closest element in $\sset{x_1, \ldots, x_n}$.

    \citet{wuExplicitNeuralNetwork2018} explicitly construct a two-layer neural network $\eta:\calX \to \{0, 1\}^{n}$ such that
    \[{(\eta(x))}_i = 1 \iff x \in V^{(i)}.\]
    The construction contains $n (n - 1)$ neurons in the first layer and $n$ neurons in the second layer, both with the threshold function $\sigma(x) = \bm{1}\sset{x \geq 0}$ as the activation function. Note that sigmoidal activation functions can approximate the step function arbitrarily well. The weights between the first and second layer are sparse. The neural network takes the form,
    \begin{equation*}
        \begin{aligned}
            z_{k,j}^{(1)}(x) &= \sigma\paren{w_{k,j}^{(1)} \cdot x - b_{k,j}^{(1)}}, \quad k,j \in [n], k \neq j\\
            z_k^{(2)}(x) &= \sigma\paren{w_k^{(1)} \cdot \bm{z}^{(1)}(x) - b^{(2)}}, \quad k \in [n] \\
            \eta(x) &= \bm{z}^{(2)}(x) = \paren{z_1^{(2)}(x), \ldots, z_n^{(2)}(x)},
        \end{aligned}
    \end{equation*}
    where the weights are $w_{k,j}^{(1)} = x_k - x_j, b_{k,j}^{(1)} = \frac{1}{2} \iiprod{x_k - x_j}{x_k + x_j}, (w_{k}^{(2)})_{a,b} = \bm{1}\sset{a = k}, b^{(2)} = n - 1$, and $\bm{z}^{(1)} = (z_{k,j}^{(1)})_{k \neq j} \in \reals^{n (n - 1)}$ are the first layer activations.

    Let $R \in \reals^{n \times n}$ be defined by $\bbra{R}_{ij} = r(x_i, x_j)$. The matrix $R$ specifies the relation function on the sample points $x_1, \ldots, x_n$. We have that $R = P^\top Q,\, P,Q \in \reals^{m \times n}$ for some $m \leq n$. 

    Let $\phi = P \circ \eta$ and $\psi = Q \circ \eta$. We will show that the inner product of neural networks $\iprod{\phi(x)}{\psi(y)}$ approximates $r(x, y)$. In fact, this is immediate. Take $x, y \in \calX$. We have
    \begin{align*}
        \abs{r(x, y) - \iprod{\phi(x)}{\psi(y)}} &\leq \abs{r(x, y) - r(q(x), q(y))} + \abs{r(q(x), q(y)) - \iprod{\phi(x)}{\phi(y)}}\\
        &\leq \epsilon + \abs{r(q(x), q(y)) - \eta(x)^\top R\, \eta(y)},
    \end{align*}
    where the second inequality follows by the assumption of continuity on the relation function $r$ and the choice of the quantization diameter $\delta$. Now, note that whenever $x \in \mathrm{int}(V^{(i)})$, we have $\eta(x) = e_i$, where $e_i$ is the canonical basis vector. Hence $r(q(x), q(y)) = \eta(x)^\top R\, \eta(y)$ Lebesgue-almost everywhere. This completes the proof.
\end{proof}

\begin{remark}
    From a learning perspective, we only need to know the value of the relation function at $n$ (uniformly distributed) points $x_1, \ldots, x_n$, with $n = \calO(\delta^{-d})$ depending on the smoothness of $r$ and the dimension of the space $\calX$.
\end{remark}

\begin{remark}
    In the symmetric case, where the relation function $r$ is assumed to be a positive-definite kernel, Mercer's theorem implies an inner product-like structure in $r$, with a ``rank'' determined by the spectral decay of the kernel. In the asymmetric case, the above result assumes only the continuity of the relation function $r$ and no further ``inner-product-like'' structure. In some applications, the relation function may have a ``low-rank'' structure such that it is representable by an inner product between low-dimensional feature maps, enabling more favorable bounds on the size of the neural network needed, even in the asymmetric case.
\end{remark}

%% BELOW IS AN OLD VERSION OF THIS SECTION WHERE THE CONSTRUCTION IS BASED ON 

% In the previous section we considered symmetric inner product relations where the encoder of the first object is the same as the encoder of the second object. When the underlying relation being modeled is a symmetric `similarity' relation, this is a useful inductive bias. However, in general, relations between objects can be asymmetric. For example, order relations are asymmetric (in fact, anti-symmetric). Such relations cannot be captured by symmetric inner products. In this section, we consider modeling a general (asymmetric) relation $r: \calX \times \calX$ as the inner product of two different neural network encodings of a pair of objects,
% \begin{equation}\label{eq:asymmetric_iprod_relation}
%     r(x, y) = \iprod{\phi(x)}{\psi(y)},
% \end{equation}
% where $\phi, \psi: \calX \to \reals^d$ are two neural networks.

% In this section, we show that inner products of universal approximators (e.g., when $\phi, \psi$ are multi-layer perceptrons) can approximate any continuous function on $\calX \times \calX$.

% We begin with the following simple lemma which states when the object space $\calX$ is finite, any relation function can be represented as the inner product between two encodings.

% \begin{lemma}\label{lemma:finite_space_rel}
%     Suppose $\calX$ is a finite space. Let $r: \calX \times \calX \to \reals$ be any relation function. Then, there exists $d \leq \abs{\calX}$ and $\phi, \psi: \calX \to \reals^{d}$ such that,
%     \begin{equation*}
%         r(x, y) = \iprod{\phi(x)}{\psi(y)}, \ \forall x, y \in \calX.
%     \end{equation*}
% \end{lemma}

% \begin{proof}
%     Let $x_1, \ldots, x_m$ be an enumeration of $\calX$ where $m = \abs{\calX}$. Let $R \in \reals^{m \times m}$ such that $R_{ij} = r(x_i, x_j)$. There exists many decompositions of the matrix $R$ which would induce valid encodings $\phi, \psi$. One example is rank decomposition. Let $d = \mathrm{rank}(R)$. Then, there exists matrices $P, Q \in \reals^{d \times m}$ such that $R = P^\top Q$. Let $\phi, \psi: \calX \to \reals^{d}$ be defined by
%     \begin{equation}
%         \phi(x_i) = P_{i, \cdot}, \ \psi(x_i) = Q_{\cdot, i}, \ \forall i \in [m].
%     \end{equation}

%     Then, $r(x, y) = \iprod{\phi(x)}{\psi(y)}$ for all $x, y \in \calX$.
% \end{proof}

% Note that if each $x \in \calX$ is a one-hot vector in $\reals^{\abs{\calX}}$, then the result above holds with linear maps $\phi, \psi$. Although very simple, this result has direct implications to domains such as language modeling where $\calX$ is a discrete set of tokens, and hence finite. In such cases,~\Cref{lemma:finite_space_rel} tells us that any relation function can be approximated by inner products of feature maps (i.e., of the form present in the attention mechanisms of Transformers). Moreover, in the case of language, there may be a low-rank structure (e.g., depending on syntax, semantics, etc.) enabling a more modest dimension of the feature maps, $d \ll \abs{\calX}$.

% Next, we proceed to show that arbitrary continuous relation functions can be approximated by inner products of two different neural networks. For simplicity, we assume $\calX = [0, 1]^{d}$, where $d = \dim(\calX)$.

% \textbf{Preliminaries.} Let $\calR(\delta) = \sset{R_1, \ldots, R_{\aabs{\calR}}}$ be a partition of $\calX$ into rectangles $R_i = \times_{j=1}^{d} [a_j^i, b_j^i)$ such that $\mathrm{length}(R_i) \leq \delta$ for all $i$. Note that such partition exists with $\delta^{-d}$ rectangles. Let $\norm{\cdot}_\calF$ be a norm on the space of functions from $\calX$ to $\reals$. In particular, we consider the sup-norm $\nnorm{f(\cdot)}_\calF = \sup_{x \in \calX} \abs{f(x)}$ or $L^p$-norms $\nnorm{f(\cdot)}_\calF = \pparen{\int_\calX \abs{f(x)}^p d\mu(x)}^{1/p}$ for some probability measure $\mu$. We denote the corresponding norm on functions from $\calX \times \calX$ to $\reals$ as $\nnorm{f(\cdot)}_{\calF^\otimes 2}$.%, defined as in~\Cref{sec:symmetric_relations}.

% Now consider the quantization function which maps $\calX$ to one of the rectangles in $\calR$. In particular, for a partition of rectangles $\calR(\delta)$, let $q_\delta: \calX \to \reals^{\aabs{\calR}}$ be defined as $q_\delta = \pparen{\bm{1}_{R_1}, \ldots, \bm{1}_{R_{\aabs{\calR}}}}$. That is, for each dimension, it is the indicator of whether the input lies in the corresponding rectangle. We will be interested in approximating $q_\delta$ by a neural network. For a class of neural networks $\calV_N$ with complexity (i.e., total number of units) $N$, let $\mathrm{dist}(q_\delta, \calV_N) = \inf_{h \in \calV_N} \nnorm{q_\delta - h}_\calF$. Let $\calN(\epsilon) \in \naturals$ be the integer such that $\mathrm{dist}(q_\delta, \calV_N) \leq \epsilon$ if $N \geq \calN(\epsilon; \calV)$. We assume that $\calV_N \subset \calV_{N+1}$.

% The following theorem states that any continuous relation function can be approximated by an inner product of two neural networks. The proof strategy will be to quantize the input into rectangles using a neural network, then use~\Cref{lemma:finite_space_rel}.

% \begin{theorem}\label{theorem:asymemtric_inner_prod_rel_func_class}
%     Suppose the relation function $r: \calX \times \calX \to \reals$ is continuous. In particular, for any $\epsilon > 0$, there exists $\delta(\epsilon) > 0$ such that for any $x, y, \tilde{x}, \tilde{y}$ satisfying $\infnorm{x - \tilde{x}} \leq \delta$, $\infnorm{y - \tilde{y}} \leq \delta$, we have $\abs{r(x, y) - r(\tilde{x}, \tilde{y})} \leq \epsilon$. Then, for any approximation error $\epsilon > 0$ there exists multi-layer perceptrons $\phi, \psi$ such that
%     \begin{equation*}
%         \norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \epsilon.
%     \end{equation*}
%     Moreover, $\phi, \psi$ can be constructed such that $\phi = L_\phi \circ \hat{q}$, $\psi = L_\psi \circ \hat{q}$ where $\hat{q} \in \calV_N$ is a shared MLP with $N$ neurons and $L_\phi, L_\psi$ are linear projections onto $\delta^{-d}$-dimensional space. The number of neurons is bounded by $N = \calN(\tilde{\epsilon}; \calV)$, where $\tilde{\epsilon} = \pparen{\sqrt{2 \delta^{-d} R_{\max} (2 R_{\max} + \epsilon)} - 2 \delta^{-d / 2} R_{\max}} \pparen{2 \delta^{-d} R_{\max}}^{-1}$,
%     $R_{\max} \coloneq \max_{x,y} r(x,y)$, and $\delta = \delta(\epsilon / 2)$.
% \end{theorem}
% \begin{proof}
%     Let $\delta = \delta(\epsilon / 2)$, and let $\calR(\delta) = {R_1, \ldots, R_{\aabs{\calX}}}$ be a partition of $\calX$ into rectangles, with $\abs{\calR} \leq \delta^{-d}$. Recall that $q_\delta$ is defined as the quantizer of $\calX$ defined in terms of indicators of the rectangles in $\calR$. Let $N = \calN(\tilde{\epsilon}; \calV)$ and let $\hat{q}_\delta \in \calV_N$ be a neural network such that $\norm{\hat{q}_\delta - q_\delta}_\calF \leq \tilde{\epsilon}$, where $\tilde{\epsilon} > 0$ will be determined later. Let $x_1, \ldots, x_{\aabs{\calR}}$ be the midpoints of the rectangles in $\calR$ such that $x_i \in R_i,\, i \in [\aabs{\calR}]$. Let $R \in \reals^{\aabs{\calR} \times \aabs{\calR}}$ be defined by $\bbra{R}_{ij} = r(x_i, x_j)$. We have that $R = P^\top Q,\, P,Q \in \reals^{r \times \aabs{\calR}}$ for some $r \leq \delta^{-d}$.

%     Let $\phi = P \circ \hat{q}_\delta$ and $\psi = Q \circ \hat{q}_{\delta}$. We will show that the inner product of neural networks $\iprod{\phi(x)}{\psi(y)}$ approximates $r(x, y)$. For convenience, we denote $q_\delta(x)$ by $q_\delta(x)$ and $\hat{q}_\delta(x)$ by $\hat{q}_\delta(x)$ in the calculation below.
%     \begin{align*}
%         &\norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \\
%         &\leq \norm{r(x, y) - r(q_\delta(x), q_\delta(y))}_{\calF^{\otimes 2}} + \norm{r(q_\delta(x), q_\delta(y)) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}}\\
%         &\stepa{\leq} \frac{\epsilon}{2} + \norm{q_{\delta}(x)^\top R\, q_{\delta}(y) - \hat{q}_{\delta}(x)^\top R\, \hat{q}_{\delta}(y)}_{\calF^{\otimes 2}}\\
%         &\stepb{\leq} \frac{\epsilon}{2} + \norm{(q_\delta(x) - \hat{q}_\delta(x))^\top R\, q_\delta(y)}_{\calF^{\otimes 2}} + \norm{\hat{q}_\delta(x)^\top R\, (q_\delta(y) - \hat{q}_\delta(y))}_{\calF^{\otimes 2}}
%     \end{align*}
%     In the above, step (a) is by the assumption of continuity of $r: \calX \times \calX \to \reals$ and the definition of the matrix $R$ and the quantizer $q_\delta$. Step (b) is the triangle inequality.

%     We proceed to bound each of the two terms above. The first term is bounded as follows
%     \begin{align*}
%         \abs{(q_\delta(x) - \hat{q}_\delta(x))^\top R\, q_\delta(y)} &\leq \twonorm{q_\delta(x) - \hat{q}_\delta(x)} \twonorm{q_\delta(y)} \norm{R}_{2 \mapsto 2}\\
%         &\leq \twonorm{q_\delta(x) - \hat{q}_\delta(x)} \delta^{-\delta_\calX} R_{\max},
%     \end{align*}
%     where we define $R_{\max} \coloneq \max_{x,y} \aabs{r(x,y)}$ and the second inequality follows by noting that the two-norm of a matrix $A \in\reals^{m \times n}$ is bounded by $\norm{A}_{2 \mapsto 2} \leq \sqrt{m n} \norm{A}_{\max}$. The second term is bounded by first noting that
%     \begin{align*}
%         \abs{\hat{q}_\delta(x)^\top R\, (q_\delta(y) - \hat{q}_\delta(y))} &\leq \twonorm{\hat{q}_\delta(x)^\top R} \twonorm{q_\delta(y) - \hat{q}_\delta(y)} \\
%         &\leq \paren{\twonorm{(\hat{q}_\delta(x) - q_\delta(x))^\top R} + \twonorm{q_\delta(x) R}} \twonorm{q_\delta(y) - \hat{q}_\delta(y)}\\
%         &\leq \paren{\norm{R}_{2 \mapsto 2} \twonorm{\hat{q}_\delta(x) - q_\delta(x)}+ \max_i \twonorm{R_{i\cdot}}} \twonorm{q_\delta(y) - \hat{q}_\delta(y)}\\
%         &\leq \delta^{-d} R_{\max} \twonorm{\hat{q}_\delta(x) - q_\delta(x)} \twonorm{q_\delta(y) - \hat{q}_\delta(y)} + \delta^{-d / 2} R_{\max} \twonorm{q_\delta(y) - \hat{q}_\delta(y)}.
%     \end{align*}
%     Hence,
%     \begin{align*}
%         \norm{\hat{q}_\delta(x)^\top R\, (q_\delta(y) - \hat{q}_\delta(y))}_{\calF^{\otimes 2}} &\leq  \delta^{-d} R_{\max} \norm{\twonorm{\hat{q}_\delta - q_\delta}}_{\calF}^2 + \delta^{-d / 2} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}.
%     \end{align*}

%     Putting this together and continuing with the bound, we obtain
%     \begin{align*}
%         &\norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \frac{\epsilon}{2} + 2 \delta^{- d / 2} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF} + \delta^{- d} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}^2.
%     \end{align*}

%     Next, we choose the neural network complexity $N$ such that the above is bounded by $\epsilon$. Vt considering the above as a quadratic in $\norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}$, we observe that this holds when
%     \begin{equation*}
%         \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF} \in \Big(0, \frac{\sqrt{2 \delta^{-d} R_{\max} (2 R_{\max} + \epsilon)} - 2 \delta^{-d / 2} R_{\max}}{2 \delta^{-d} R_{\max}} \Big].
%     \end{equation*}
%     This can be seen by considering the quadratic function in the error $\nnorm{\twonorm{q_\delta - \hat{q}_\delta}}_\calF$ and finding values for which the overall error is bounded by $\epsilon$. Hence, letting
%     \[\tilde{\epsilon} \coloneq \frac{\sqrt{2 \delta^{-d} R_{\max} (2 R_{\max} + \epsilon)} - 2 \delta^{-d / 2} R_{\max}}{2 \delta^{-d} R_{\max}}\]
%     and $N = \calN(\tilde{\epsilon}; \calV)$, there exists a neural network $\hat{q}_\delta \in \calV_N$ such that
%     \begin{align*}
%         \norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \epsilon
%     \end{align*}
% \end{proof}

% \aanote*{An alternate bound on $\tilde{\epsilon}$... more interpretable but less tight. should we use this instead?}{
% Consider the proof starting from here...
% \begin{equation*}
%     \norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \frac{\epsilon}{2} + 2 \delta^{- d / 2} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF} + \delta^{- d} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}^2
% \end{equation*}
% We will try to get a different, more interpretable bound on $\tilde{\epsilon}$. Let $\tilde{\epsilon} := \nnorm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}$ and consider the quadratic $2 \delta^{-d/2} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon}^2 + \epsilon / 2$. We want to find values of $\tilde{\epsilon}$ such that this is bounded by $\epsilon$.

% Assume that $\delta = \min(\delta(\epsilon / 2), 1)$ and $\tilde{\epsilon} \leq 1$. Then,
% \begin{align*}
%     2 \delta^{-d/2} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon}^2 + \epsilon / 2 &\leq 2 \delta^{-d/2} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon} + \epsilon / 2\\
%     &\leq 2 \delta^{-d} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon} + \epsilon / 2\\
%     &= 3 \delta^{-d} R_{\max} \tilde{\epsilon} + \epsilon / 2
% \end{align*}
% where the first inequality is since $\tilde{\epsilon} \leq 1$ and the second is because $\delta \leq 1$. Hence, the expression is less than $\epsilon$ when $\tilde{\epsilon} = \min((6 R_{\max})^{-1} \delta^d \epsilon, 1)$.

% }
\aanote{[todo]: add discussion/section on the scale of NNs needed to approximate a particular relation function.}

\aanote{can we show that $\delta^{-\dim(\calX)}$ is necessary in the worst case? can make use of lower bounds from standard MLPs.}

\aanote{
    this universal approximation is based on arbitrary continuous functions on the $d$-dimensional space. but, in practice, we may be interested in relations with ``low-rank structure'' (i.e., relations in low-dimensional spaces). e.g., $\calX$ may be a space of images (e.g., $d =  l \times w \times 3$), but we may be only interested in computing relations between an attribute like color, texture, or shape of a particular object in the image, which  would be representable in a much lower-dimensional space (e.g., $d = 3$). Moreover, `projections' onto the low-dimensional space on which to compute relations may be very smooth or even linear. e.g., $r(x, y) = \bar{r}(\phi(x), \psi(y))$ where $\phi, \psi$ are smooth and low-dimensional. then, we can get something like $\delta^{-k}$ to approximate $\bar{r}$ and $k \cdot \calN_\calF(\epsilon)$ to approximate $\phi, \psi$ which we can assume are smooth. can we formalize this and develop a theory. i.e., formalize an assumption on the relation function as a composition of a smooth projection into a lower dimensional space followed by a continuous bivariate function. can note relation to how `relations' are computed in Transformers/Abstractors/RelConvNet, etc. first a FeedForward net, then low-dimensional projections.
    }