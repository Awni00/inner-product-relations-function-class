\section{Function class of asymmetric inner product relations}\label{sec:asymmetric_relations}

In the previous section we considered symmetric inner product relations where the encoder of the first object is the same as the encoder of the second object. When the underlying relation being modeled is a symmetric `similarity' relation, this is a useful inductive bias. However, in general, relations between objects can be asymmetric. For example, order relations are asymmetric (in fact, anti-symmetric). Such relations cannot be captured by symmetric inner products. In this section, we consider modeling a general (asymmetric) relation $r: \calX \times \calX$ as the inner product of two different neural network encodings of a pair of objects,
\begin{equation}\label{eq:asymmetric_iprod_relation}
    r(x, y) = \iprod{\phi(x)}{\psi(y)},
\end{equation}
where $\phi, \psi: \calX \to \reals^d$ are two neural networks.

In this section, we show that inner products of universal approximators (e.g., when $\phi, \psi$ are multi-layer perceptrons) can approximate any continuous function on $\calX \times \calX$.

We begin with the following simple lemma which states when the object space $\calX$ is finite, any relation function can be represented as the inner product between two encodings.

\begin{lemma}\label{lemma:finite_space_rel}
    Suppose $\calX$ is a finite space. Let $r: \calX \times \calX \to \reals$ be any relation function. Then, there exists $d \leq \abs{\calX}$ and $\phi, \psi: \calX \to \reals^{d}$ such that,
    \begin{equation*}
        r(x, y) = \iprod{\phi(x)}{\psi(y)}, \ \forall x, y \in \calX.
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $x_1, \ldots, x_m$ be an enumeration of $\calX$ where $m = \abs{\calX}$. Let $R \in \reals^{m \times m}$ such that $R_{ij} = r(x_i, x_j)$. There exists many decompositions of the matrix $R$ which would induce valid encodings $\phi, \psi$. One example is rank decomposition. Let $d = \mathrm{rank}(R)$. Then, there exists matrices $P, Q \in \reals^{d \times m}$ such that $R = P^\top Q$. Let $\phi, \psi: \calX \to \reals^{d}$ be defined by
    \begin{equation}
        \phi(x_i) = P_{i, \cdot}, \ \psi(x_i) = Q_{\cdot, i}, \ \forall i \in [m].
    \end{equation}

    Then, $r(x, y) = \iprod{\phi(x)}{\psi(y)}$ for all $x, y \in \calX$.
\end{proof}

Note that if each $x \in \calX$ is a one-hot vector in $\reals^{\abs{\calX}}$, then the result above holds with linear maps $\phi, \psi$. Although very simple, this result has direct implications to domains such as language modeling where $\calX$ is a discrete set of tokens, and hence finite. In such cases,~\Cref{lemma:finite_space_rel} tells us that any relation function can be approximated by inner products of feature maps (i.e., of the form present in the attention mechanisms of Transformers). Moreover, in the case of language, there may be a low-rank structure (e.g., depending on syntax, semantics, etc.) enabling a more modest dimension of the feature maps, $d \ll \abs{\calX}$.

Next, we proceed to show that arbitrary continuous relation functions can be approximated by inner products of two different neural networks. For simplicity, we assume $\calX = [0, 1]^{d}$, where $d = \mathrm{dim}(\calX)$.

\textbf{Preliminaries.} Let $\calR(\delta) = \sset{R_1, \ldots, R_{\aabs{\calR}}}$ be a partition of $\calX$ into rectangles $R_i = \times_{j=1}^{d} [a_j^i, b_j^i)$ such that $\mathrm{length}(R_i) \leq \delta$ for all $i$. Note that such partition exists with $\delta^{-d}$ rectangles. Let $\norm{\cdot}_\calF$ be a norm on the space of functions from $\calX$ to $\reals$. In particular, we consider the sup-norm $\nnorm{f(\cdot)}_\calF = \sup_{x \in \calX} \abs{f(x)}$ or $L^p$-norms $\nnorm{f(\cdot)}_\calF = \pparen{\int_\calX \abs{f(x)}^p d\mu(x)}^{1/p}$ for some probability measure $\mu$. We denote the corresponding norm on functions from $\calX \times \calX$ to $\reals$ as $\nnorm{f(\cdot)}_{\calF^\otimes 2}$.%, defined as in~\Cref{sec:symmetric_relations}.

Now consider the quantization function which maps $\calX$ to one of the rectangles in $\calR$. In particular, for a partition of rectangles $\calR(\delta)$, let $q_\delta: \calX \to \reals^{\aabs{\calR}}$ be defined as $q_\delta = \pparen{\bm{1}_{R_1}, \ldots, \bm{1}_{R_{\aabs{\calR}}}}$. That is, for each dimension, it is the indicator of whether the input lies in the corresponding rectangle. We will be interested in approximating $q_\delta$ by a neural network. For a class of neural networks $\calV_N$ with complexity (i.e., total number of units) $N$, let $\mathrm{dist}(q_\delta, \calV_N) = \inf_{h \in \calV_N} \nnorm{q_\delta - h}_\calF$. Let $\calN(\epsilon) \in \naturals$ be the integer such that $\mathrm{dist}(q_\delta, \calV_N) \leq \epsilon$ if $N \geq \calN(\epsilon; \calV)$. We assume that $\calV_N \subset \calV_{N+1}$.

The following theorem states that any continuous relation function can be approximated by an inner product of two neural networks. The proof strategy will be to quantize the input into rectangles using a neural network, then use~\Cref{lemma:finite_space_rel}.

\begin{theorem}\label{theorem:asymemtric_inner_prod_rel_func_class}
    Suppose the relation function $r: \calX \times \calX \to \reals$ is continuous. In particular, for any $\epsilon > 0$, there exists $\delta(\epsilon) > 0$ such that for any $x, y, \tilde{x}, \tilde{y}$ satisfying $\infnorm{x - \tilde{x}} \leq \delta$, $\infnorm{y - \tilde{y}} \leq \delta$, we have $\abs{r(x, y) - r(\tilde{x}, \tilde{y})} \leq \epsilon$. Then, for any approximation error $\epsilon > 0$ there exists multi-layer perceptrons $\phi, \psi$ such that
    \begin{equation*}
        \norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \epsilon.
    \end{equation*}
    Moreover, $\phi, \psi$ can be constructed such that $\phi = L_\phi \circ \hat{q}$, $\psi = L_\psi \circ \hat{q}$ where $\hat{q} \in \calV_N$ is a shared MLP with $N$ neurons and $L_\phi, L_\psi$ are linear projections onto $\delta^{-d}$-dimensional space. The number of neurons is bounded by $N = \calN(\tilde{\epsilon}; \calV)$, where $\tilde{\epsilon} = \pparen{\sqrt{2 \delta^{-d} R_{\max} (2 R_{\max} + \epsilon)} - 2 \delta^{-d / 2} R_{\max}} \pparen{2 \delta^{-d} R_{\max}}^{-1}$,
    $R_{\max} \coloneq \max_{x,y} r(x,y)$, and $\delta = \delta(\epsilon / 2)$.
\end{theorem}
\begin{proof}
    Let $\delta = \delta(\epsilon / 2)$, and let $\calR(\delta) = {R_1, \ldots, R_{\aabs{\calX}}}$ be a partition of $\calX$ into rectangles, with $\abs{\calR} \leq \delta^{-d}$. Recall that $q_\delta$ is defined as the quantizer of $\calX$ defined in terms of indicators of the rectangles in $\calR$. Let $N = \calN(\tilde{\epsilon}; \calV)$ and let $\hat{q}_\delta \in \calV_N$ be a neural network such that $\norm{\hat{q}_\delta - q_\delta}_\calF \leq \tilde{\epsilon}$, where $\tilde{\epsilon} > 0$ will be determined later. Let $x_1, \ldots, x_{\aabs{\calR}}$ be the midpoints of the rectangles in $\calR$ such that $x_i \in R_i,\, i \in [\aabs{\calR}]$. Let $R \in \reals^{\aabs{\calR} \times \aabs{\calR}}$ be defined by $\bbra{R}_{ij} = r(x_i, x_j)$. We have that $R = P^\top Q,\, P,Q \in \reals^{r \times \aabs{\calR}}$ for some $r \leq \delta^{-d}$.

    Let $\phi = P \circ \hat{q}_\delta$ and $\psi = Q \circ \hat{q}_{\delta}$. We will show that the inner product of neural networks $\iprod{\phi(x)}{\psi(y)}$ approximates $r(x, y)$. For convenience, we denote $q_\delta(x)$ by $q_\delta(x)$ and $\hat{q}_\delta(x)$ by $\hat{q}_\delta(x)$ in the calculation below.
    \begin{align*}
        &\norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \\
        &\leq \norm{r(x, y) - r(q_\delta(x), q_\delta(y))}_{\calF^{\otimes 2}} + \norm{r(q_\delta(x), q_\delta(y)) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}}\\
        &\stepa{\leq} \frac{\epsilon}{2} + \norm{q_{\delta}(x)^\top R\, q_{\delta}(y) - \hat{q}_{\delta}(x)^\top R\, \hat{q}_{\delta}(y)}_{\calF^{\otimes 2}}\\
        &\stepb{\leq} \frac{\epsilon}{2} + \norm{(q_\delta(x) - \hat{q}_\delta(x))^\top R\, q_\delta(y)}_{\calF^{\otimes 2}} + \norm{\hat{q}_\delta(x)^\top R\, (q_\delta(y) - \hat{q}_\delta(y))}_{\calF^{\otimes 2}}
    \end{align*}
    In the above, step (a) is by the assumption of continuity of $r: \calX \times \calX \to \reals$ and the definition of the matrix $R$ and the quantizer $q_\delta$. Step (b) is the triangle inequality.

    We proceed to bound each of the two terms above. The first term is bounded as follows
    \begin{align*}
        \abs{(q_\delta(x) - \hat{q}_\delta(x))^\top R\, q_\delta(y)} &\leq \twonorm{q_\delta(x) - \hat{q}_\delta(x)} \twonorm{q_\delta(y)} \norm{R}_{2 \mapsto 2}\\
        &\leq \twonorm{q_\delta(x) - \hat{q}_\delta(x)} \delta^{-\delta_\calX} R_{\max},
    \end{align*}
    where we define $R_{\max} \coloneq \max_{x,y} \aabs{r(x,y)}$ and the second inequality follows by noting that the two-norm of a matrix $A \in\reals^{m \times n}$ is bounded by $\norm{A}_{2 \mapsto 2} \leq \sqrt{m n} \norm{A}_{\max}$. The second term is bounded by first noting that
    \begin{align*}
        \abs{\hat{q}_\delta(x)^\top R\, (q_\delta(y) - \hat{q}_\delta(y))} &\leq \twonorm{\hat{q}_\delta(x)^\top R} \twonorm{q_\delta(y) - \hat{q}_\delta(y)} \\
        &\leq \paren{\twonorm{(\hat{q}_\delta(x) - q_\delta(x))^\top R} + \twonorm{q_\delta(x) R}} \twonorm{q_\delta(y) - \hat{q}_\delta(y)}\\
        &\leq \paren{\norm{R}_{2 \mapsto 2} \twonorm{\hat{q}_\delta(x) - q_\delta(x)}+ \max_i \twonorm{R_{i\cdot}}} \twonorm{q_\delta(y) - \hat{q}_\delta(y)}\\
        &\leq \delta^{-d} R_{\max} \twonorm{\hat{q}_\delta(x) - q_\delta(x)} \twonorm{q_\delta(y) - \hat{q}_\delta(y)} + \delta^{-d / 2} R_{\max} \twonorm{q_\delta(y) - \hat{q}_\delta(y)}.
    \end{align*}
    Hence,
    \begin{align*}
        \norm{\hat{q}_\delta(x)^\top R\, (q_\delta(y) - \hat{q}_\delta(y))}_{\calF^{\otimes 2}} &\leq  \delta^{-d} R_{\max} \norm{\twonorm{\hat{q}_\delta - q_\delta}}_{\calF}^2 + \delta^{-d / 2} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}.
    \end{align*}

    Putting this together and continuing with the bound, we obtain
    \begin{align*}
        &\norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \frac{\epsilon}{2} + 2 \delta^{- d / 2} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF} + \delta^{- d} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}^2.
    \end{align*}

    Next, we choose the neural network complexity $N$ such that the above is bounded by $\epsilon$. Vt considering the above as a quadratic in $\norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}$, we observe that this holds when
    \begin{equation*}
        \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF} \in \Big(0, \frac{\sqrt{2 \delta^{-d} R_{\max} (2 R_{\max} + \epsilon)} - 2 \delta^{-d / 2} R_{\max}}{2 \delta^{-d} R_{\max}} \Big].
    \end{equation*}
    This can be seen by considering the quadratic function in the error $\nnorm{\twonorm{q_\delta - \hat{q}_\delta}}_\calF$ and finding values for which the overall error is bounded by $\epsilon$. Hence, letting
    \[\tilde{\epsilon} \coloneq \frac{\sqrt{2 \delta^{-d} R_{\max} (2 R_{\max} + \epsilon)} - 2 \delta^{-d / 2} R_{\max}}{2 \delta^{-d} R_{\max}}\]
    and $N = \calN(\tilde{\epsilon}; \calV)$, there exists a neural network $\hat{q}_\delta \in \calV_N$ such that
    \begin{align*}
        \norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \epsilon
    \end{align*}
\end{proof}

\aanote*{An alternate bound on $\tilde{\epsilon}$... more interpretable but less tight. should we use this instead?}{
Consider the proof starting from here...
\begin{equation*}
    \norm{r(x,y) - \iprod{\phi(x)}{\psi(y)}}_{\calF^{\otimes 2}} \leq \frac{\epsilon}{2} + 2 \delta^{- d / 2} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF} + \delta^{- d} R_{\max} \norm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}^2
\end{equation*}
We will try to get a different, more interpretable bound on $\tilde{\epsilon}$. Let $\tilde{\epsilon} := \nnorm{\twonorm{q_\delta - \hat{q}_\delta}}_{\calF}$ and consider the quadratic $2 \delta^{-d/2} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon}^2 + \epsilon / 2$. We want to find values of $\tilde{\epsilon}$ such that this is bounded by $\epsilon$.

Assume that $\delta = \min(\delta(\epsilon / 2), 1)$ and $\tilde{\epsilon} \leq 1$. Then,
\begin{align*}
    2 \delta^{-d/2} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon}^2 + \epsilon / 2 &\leq 2 \delta^{-d/2} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon} + \epsilon / 2\\
    &\leq 2 \delta^{-d} R_{\max} \tilde{\epsilon} + \delta^{-d} R_{\max} \tilde{\epsilon} + \epsilon / 2\\
    &= 3 \delta^{-d} R_{\max} \tilde{\epsilon} + \epsilon / 2
\end{align*}
where the first inequality is since $\tilde{\epsilon} \leq 1$ and the second is because $\delta \leq 1$. Hence, the expression is less than $\epsilon$ when $\tilde{\epsilon} = \min((6 R_{\max})^{-1} \delta^d \epsilon, 1)$.

}

\aawarning{[TODO]: add corollary based on approximating indicators of rectangles.}
