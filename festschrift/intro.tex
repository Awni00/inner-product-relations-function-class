\section{Introduction}\label{sec:intro}


In this paper, we study the ability of neural networks to approximate relation functions, which lie at the heart of many machine learning frameworks. In general, machine learning systems must be able to represent and reason about relations between objects, either explicitly or implicitly. For example, a natural language understanding system takes a sequence of words as input and extracts information about the meaning and function of the words based on the local context. Similarly, a scene analysis system considers the spatial and visual relationship between the components of a scene in order to identify and interpret the implicit objects. As a simple example, in a scene where a sphere sits on top of a cube, where the sphere is blue and the cube is red, the visual AI system might be required to reason that the red object is supporting the blue object. Likewise, an NLP system might make this inference by analyzing the syntax and semantics of the preceding sentence, extracting one relation based on color, another based on position.

A common way to represent relations between objects is through inner products between feature representations of the form $\iprod{\phi(x)}{\psi(y)}$, where $x, y \in \calX$ are two objects and $\phi, \psi$ are neural network feature maps. Inner products possess properties that make them useful measures of similarity. The aim of this paper is to understand the representational power of this model by characterizing the class of relation functions $r: \calX \times \calX \to \reals$ that can be represented as inner products of neural networks.

The use of inner products between feature maps is widespread in machine learning architectures. 
For example, Siamese networks consist of two identical copies of a neural network, with shared parameters~\parencite{rumelhartLearningRepresentationsBackpropagating1986,langTimedelayNeuralNetwork1988,bromleySignatureVerificationUsing1993,baldiNeuralNetworksFingerprint1993,chopraLearningSimilarityMetric2005,kochSiameseNeuralNetworks2015}. Each network independently processes inputs to produce feature vectors that are then compared using some distance metric, determining the similarity or dissimilarity between the inputs. If the distance is the Euclidean distance, then 
$$\twonorm{\phi(x) - \phi(y)}^2 = \iprod{\phi(x)}{\phi(x)} + \iprod{\phi(y)}{\phi(y)} - 2 \iprod{\phi(x)}{\phi(y)},$$ 
where the inner product of neural networks arises.

A broad class of recent examples comes from the attention mechanism underlying Transformer models for sequence modeling~\parencite{vaswani2017attention}. In the Transformer, self-attention is implemented as
\begin{equation*}
    \begin{split}
        \alpha_{ij} &= \mathrm{Softmax}\paren{\bra{{\iprod{\phi_q(x_i)}{\phi_k(x_j)}}}_{j \in [n]}}_j\\
        x_i &\gets \sum_{j=1}^{n} \alpha_{ij} \phi_v(x_j)
    \end{split}
\end{equation*}
where $\phi_q, \phi_k$, and $\phi_v$ are learned transformations and $\iprod{\phi_q(x_i)}{\phi_k(x_j)}$ represents a relation between $x_i$ and $x_j$, determining how much $i$ ``should attend to'' $j$. A similar mechanism is at play in earlier neural architectures that implement content-addressable external memory~\parencite{gravesNeuralTuringMachines2014,gravesHybridComputingUsing2016a,pritzelNeuralEpisodicControl2017}, with the read/write operations typically being implemented using an inner product-based similarity computation followed by a softmax normalization. Attention has also been used in other architectures to model relations between different entities~\parencite{velickovicGraphAttentionNetworks2017a,santoroRelationalRecurrentNeural2018,zambaldiDeepReinforcementLearning2018a,locatelloObjectCentricLearningSlot2020b}. For example, \citet{santoroRelationalRecurrentNeural2018} propose a recurrent neural network with a memory module that employs dot product attention to allow elements of the memory store to interact.

While the Transformer models relations implicitly through its attention mechanism, inner products of features are also central to many explicitly relational neural architectures~\parencite[e.g.,][]{webbEmergentSymbols2021,kergNeuralArchitecture2022,altabaaRelationalConvolutionalNetworks2023,altabaaAbstractorsRelationalCrossattention2024,altabaa2024disentangling}. For example, in the model proposed by~\cite{kergNeuralArchitecture2022}, a similarity matrix is computed consisting of symmetric inner products between each pair of objects, $R_{i,\cdot} = \mathrm{Softmax}\pparen{\bra{\iprod{\phi(x_i)}{\phi(x_j)}}_{j\in[n]}}$.~\citet{altabaaAbstractorsRelationalCrossattention2024} propose a Transformer-based architecture imbued with relational inductive biases by replacing the values $\phi_v(x_i)$ with vector representations which identify objects but are independent of object-level features.~\citet{altabaaRelationalConvolutionalNetworks2023} propose a relational architecture where the central operation is a type of graph-structured convolution operating on a tensor of relations computed via inner products of feature maps.


In this paper we characterize the function class of inner products of neural networks, showing that inner products of neural networks are universal approximators for relation functions. Our analysis builds on and extends classical universal approximation results for neural networks \parencite[e.g.,][]{,cybenkoApproximationSuperpositions1989,barronUniversalApproximation1993}, and is related to more recent analysis of \citet{bachBreakingCurseDimensionality2016}. The following is a high-level overview of the results.

\def\highlight#1{\vskip8pt\noindent{\it\bfseries #1.\enspace}}

\highlight{Symmetric relations}  If the relation function $r$ defines a symmetric, positive-definite kernel, we show that the relation function can be approximated by inner products of neural networks as $$ \| r(x,y) - \langle \varphi(x), \varphi(y)\rangle \| \leq \varepsilon$$
where the neural network (MLP) $\varphi$ has $N$ neurons with $$ N \leq  d_r(\varepsilon/2) \, {\cal N}_\calF\left(\frac{\varepsilon}{4C(r) d_r(\varepsilon/2)}\right)$$
where $d_r$ is a measure of the spectral decay of the kernel, $C(r)$ is a bound on the eigenfunctions, and $\calN_\calF$ is a measure of the universal approximation properties of a given neural network class. The underlying function space can be identified with kernels of reproducing kernel Hilbert spaces.

This result does not make explicit the curse of dimensionality. As a corollary, we show that under natural regularity assumptions, the number of neurons scales as 
$$ N = {\mathcal O} \left(d_r(\varepsilon/2) \, \Bigl(\frac{d_r(\varepsilon/2)}{\varepsilon}\Bigr)^d\right)$$
where $d$ is the dimension of the input space $\calX$.

\highlight{Asymmetric relations} This is the setting of many applications of relational learning. In this case, the relation function can be approximated in terms of two different multi-layer perceptrons $\varphi$ and $\psi$, with approximation error 
\[\| r(x,y) - \langle \varphi(x), \psi(y)\rangle \| \leq \varepsilon\]
achieved by a pair of neural networks with a number of neurons
\[N = \calO(\delta(\epsilon)^{-2 d}),\]
where $\delta(\epsilon)$ measures the continuity of $r$ and $d$ is the dimension of the input space. The underlying function space can be identified with kernels of reproducing kernel Banach spaces.

\highlight{Attention mechanisms} Here we use universal approximation results for inner products of neural networks to show that attention mechanisms, as used in Transformers, can always retrieve the ``most relevant'' element of a set of values.  More specifically, let any query $q$ define a preorder $\preceq_q$ on the input space $\calX$ such that $x \succeq_q y$ when $x$ is more relevant to $q$ than $y$. We write $\mathrm{Select}(q, \{x_i\})$ to denote the value $x_{i^*}$ such that $x_{i^*} \succ_q x_j$ for all $j \neq i^*$. That is, the function that selects the ``maximally relevant'' element in the context. We show that there exist MLPs that define an attention mechanism satisfying 
\[\|\mathrm{Select}(q, \{x_i\}_{i \in [n]}) - \mathrm{Attention}(q, \{x_i\}_{i \in [n]})\| \leq \varepsilon\]
for any $\varepsilon$. Our formulation of this result uses the Debreu representation theorem in economics to represent preference relations in terms of utility functions.