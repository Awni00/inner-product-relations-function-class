\begin{abstract}
    In order to process contexts with several interacting objects, a machine learning model must represent and reason about relations between objects. For example, a language model must process the semantic and syntactic relations between words. A common approach towards representing relations between objects is through inner products between feature vectors produced by learnable neural networks. Modeling relations as inner products has been empirically shown to be a useful inductive bias in several relational tasks. In this paper, we study the class of relation functions generated by inner products of neural networks.
\end{abstract}