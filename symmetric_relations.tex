\section{Function class of symmetric inner product relations}\label{sec:symmetric_relations}

Consider a symmetric relation $r: \calX \times \calX \to \reals$ modeled as the inner product of neural network encodings of a pair of inputs, $x, y \in \calX$,
\begin{equation}\label{eq:symmetric_iprod_relation}
    r(x, y) = \iprod{\phi(x)}{\phi(y)},
\end{equation}

\noindent where $\phi$ is a neural network. Symmetric relation functions modeled in this way are natural \textit{measures of similarity}. Intuitively, $\phi$ extracts features of the objects $x,y$, and the inner product computes a measure of similarity between those features. To see this more formally, suppose we have a normalized inner product relation (i.e., $\phi: \calX \to \bbS^{d}$), then we have
\begin{equation}
    \twonorm{\phi(x) - \phi(y)}^2 = 2 - 2 r(x, y).
\end{equation}

The Cauchy-Schwartz inequality states that
\begin{equation}
    r(x, y)^2 \leq r(x, x) r(y, y) = 1, \ \forall x, y \in \calX.
\end{equation}

Thus, $r(x,y)$ is large and close to $1$ when $x$ and $y$ are similar (with respect to $\phi$), and close to $0$ when $x, y$ are dissimilar. Moreover, $r: \calX \times \calX \to \reals$ induces a geometry on $\calX$ with well-defined notions of distance ($d(x,y) = \sqrt{2 - 2 r(x,y)}$), angles ($\cos(\theta) = r(x,y)/\sqrt{r(x,x) r(y,y)}$), and orthogonality ($x \perp y \iff r(x,y) = 0$).

In this section, we characterize the class of relation functions which can be modeled by symmetric inner products of neural networks. We show that any continuous symmetriic positive definite kernel (i.e., Mercer kernel) can be approximated by a symmetric inner product of neural networks. This characterization is strict since, clearly $r(x,y)$ is a Mercer kernel for any $\phi$ since, $\forall g \in L^2(\calX)$,
\begin{equation*}
    \begin{split}
        \int \int g(x) r(x,y) g(y) dx dy &= \int \int g(x) \iprod{\phi(x)}{\phi(y)} g(y) dx dy \\
        &= \iprod{\int g(x) \phi(x) dx}{\int g(y) \phi(y) dy} \\
        &= \twonorm{\int g(x) \phi(x) dx} \geq 0.
    \end{split}
\end{equation*}

% COMMENT [AWNI]: is this ^ unnecessary detail? could just say this, maybe there's no need for an argument.

The following result states that if $\phi$ is a universal approximator such as a multi-layer perceptron, for any positive-definite symmetric kernel $r$, there exists a neural network $\phi$ such that the induced inner product relation approximates $r$ arbitrarily well. To state the result, we begin with some definitions.

Consider a symmetric positive definite kernel $K: \calX \times \calX \to \reals$ on a compact euclidean space $\calX$. By Mercer's theorem\parencite{mercerFunctionsPositive1909, sunMercerTheorem2005, micchelliUniversalKernels2006}, there exists non-negative eigenvalues $\set{\lambda_i}_{i=1}^{\infty}$ and continuous eigenfunctions $\set{\psi_i}_{i=1}^{\infty}$ such that for any $x,y \in \calX$, $K(x, y) = \sum_{i=1}^\infty \lambda_i \psi_i(x) \psi_i(y)$, and the convergence of the series is absolute and uniform over $\calX$. The following definition characterizes, in a sense, the complexity of a symmetric positive-definite kernel in terms its eigenfunction decomposition.

\begin{definition}[Kernel Spectrum Decay]\label{def:sym_pd_ker_specturm_decay}
	Let $K: \calX \times \calX \to \reals$ be a symmetric positive-definite kernel, with eigenvalues $\{\lambda_i\}_{i=1}^{\infty}$ and continuous eigenfunctions $\{\psi_i\}_{i=1}^{\infty}$. For $\epsilon > 0$, denote by $d_K(\epsilon) \in \naturals$ the minimum integer such that,
	\begin{equation*}
		\sup_{x, y \in \calX} \abs{K(x, y) - \sum_{i=1}^{d(\epsilon)} \lambda_i \psi_i(x) \psi_i(y)} \leq \epsilon.
	\end{equation*}
	Moreover, let $C(K) \coloneq \max_{i} \max_{x \in \calX} \abs{\sqrt{\lambda_i} \psi_i(x)}$.
\end{definition}

The quantity $d_K(\epsilon)$ associated with a kernel $K$ describes the number of eigenfunctions needed to describe $K$ up to an error of $\epsilon$. %When the kernel $K$ is fixed, we 

Next, to state our result, we define notation which describes the efficiency of a universal function approximator on a space $\calX$.
\begin{definition}[Efficiency of function approximators]\label{def:univ_approx_efficiency}
	Consider a class of neural networks on a space $\calX$ with a scalable parameter space (e.g., scalable width, depth, number of neurons, etc). For $\epsilon > 0$, denote by $\calN_\calX(\epsilon) \in \naturals$ the minimum integer such that for any continuous function $f: \calX \to \reals$, there exists a neural network in this class $g_\theta$ with $\calN_\calX(\epsilon)$ neurons that approximates $f$ uniformly on $\calX$ with error bounded by $\epsilon$, $\sup_{x \in \calX} \abs{f(x) - g_\theta(x)} \leq \epsilon$.
\end{definition}

We are now ready to formally state our result.

\begin{theorem}[Function class of symmetric inner product relational neural networks]\label{theorem:symmetric_inner_prod_rels_func_class}
	\hphantom{~}

	Suppose the data lies in a compact metric space \(\mathcal{X}\). Consider the relation model,
	\begin{equation*}
		\hat{r}(x, y) := \iprod{\phi_{\theta}(x)}{\phi_{\theta}(y)},
	\end{equation*}
	where $\phi_{\theta}: \calX \to \reals^d$ is a multi-layer perceptron with parameters $\theta$.
	For any symmetric positive-definite kernel $r: \calX \times \calX \to \reals$, there exists a multi-layer perceptron with parameters $\theta$ such that $\hat{r}$ approximates $r$ uniformly over \((x,y) \in \mathcal{X}\times\mathcal{X}\). More precisely, for all \(\epsilon > 0\), there exists a neural network with parameters $\theta$ and at most $d_r(\epsilon / 2) \cdot \calN_\calX\paren{\frac{\epsilon}{4 C(r) d_r(\epsilon / 2)}}$ neurons such that
    \[\sup_{x,y \in \mathcal{X}}{\abs{r(x,y) - \hat{r}(x,y)}} < \epsilon.\]
\end{theorem}

\begin{proof}
	By Mercer's theorem~\parencite{mercerFunctionsPositive1909, sunMercerTheorem2005, micchelliUniversalKernels2006}, there exists \((\psi_i)_{i \in \mathbb{N}}\), \(\lambda_i \geq 0\) such that \(r(x,y) = \sum_{i=1}^{\infty}{\lambda_i \psi_i(x) \psi_i(y)}\), where \(\psi_i\) and \(\lambda_i\) are eigenfunctions and eigenvalues of the integral operator
	\begin{align*}
		T_r&: L^2(\mathcal{X}) \to L^2(\mathcal{X}) \\
		T_r(f) &= \int_{\mathcal{X}}{r(\cdot, x) f(x) dx}.
	\end{align*}
	Furthermore, the convergence of the series is uniform:
	\begin{equation}
		\lim_{n \to \infty} \sup_{x,y \in \mathcal{X}} \lvert r(x,y) - \sum_{i=1}^{n}{\lambda_i \psi_i(x) \psi_i(y) \rvert} = 0
	\end{equation}
	Using the notation of~\Cref{def:sym_pd_ker_specturm_decay}, we have that
	\begin{equation}\label{eq:proof_mercer_thm_unif_abs_cv}
		\sup_{x,y \in \mathcal{X}} \left\lvert r(x,y) - \sum_{i=1}^{d_r(\epsilon/2)}{\lambda_i \psi_i(x) \psi_i(y)} \right\rvert < \frac{\epsilon}{2},
	\end{equation}
	where $d_r(\epsilon/2)$ is defined as in~\Cref{def:sym_pd_ker_specturm_decay}.

	Let $d \coloneqq d_r(\epsilon / 2)$ be the output dimension of the neural network \(\phi_\theta\) such that it maps from $\calX$ to $\reals^d$. The function to be approximated by $\phi_\theta$ is \((\sqrt{\lambda_1} \psi_1, \ldots, \sqrt{\lambda_{d}} \psi_{d})\), the first $d$ eigenfunctions of the kernel relation $r$. By the universal approximation property of multi-layer perceptrons, for any \(\epsilon_1 > 0\), there exists a neural network with parameters \(\theta\) such that
	\begin{equation}\label{eq:proof_NN_UAP}
		\sup_{x\in \mathcal{X}}{\abs{(\phi_\theta(x))_i - \sqrt{\lambda_i} \psi_i(x)}} < \epsilon_1, \ \forall i \in \{1, \ldots, d\},
	\end{equation}
	where $(\phi_\theta(x))_i$ is the \(i\)th component of $\phi_\theta(x)$. Moreover, using the notation defined in~\Cref{def:univ_approx_efficiency}, there exists such a neural network with at most $d \cdot \calN_\calX(\epsilon_1)$ neurons. Some classical results on universal approximation of neural networks are \parencite{hornikMultilayerFeedforward1989, cybenkoApproximationSuperpositions1989, barronUniversalApproximation1993}, which characterize $\calN_\calX(\epsilon_1)$. 

	Now note that the approximation error for \(r\) is bounded by
	\begin{equation}\label{eq:proof_approx_bound}
		\begin{split}
			\sup_{x, y \in \mathcal{X}}&{
				\left\lvert r(x,y) - \langle \phi_\theta(x), \phi_\theta(y) \rangle \right\rvert}\\
			&= \sup_{x, y \in \mathcal{X}}{
				\left\lvert r(x,y) - \sum_{i=1}^{d}{(\phi_\theta(x))_i (\phi_\theta(y))_i} \right\rvert} \\
			&\leq \sup_{x,y \in \mathcal{X}}{ \left(
				\left\lvert r(x,y) - \sum_{i=1}^{d}{\lambda_i \psi_i(x) \psi_i(y)} \right\rvert
				+ \left\lvert \sum_{i=1}^{d}{\lambda_i \psi_i(x) \psi_i(y) - (\phi_\theta(x))_i (\phi_\theta(y))_i} \right\rvert  \right) }
		\end{split}
	\end{equation}
	The first term is bounded by \(\frac{\epsilon}{2}\) by~\eqref{eq:proof_mercer_thm_unif_abs_cv}. The second term can be bounded uniformly on \(x,y\) by
	\begin{equation*}
		\begin{split}
			&\left\lvert \left(\sum_{i=1}^{d}{\lambda_i \psi_i(x) \psi_i(y)}\right) - \langle \phi_\theta(x), \phi_\theta(y) \rangle \right\rvert  \\
			&\leq \sum_{i=1}^{d}{ \abs{ \lambda_i \psi_i(x) \psi_i(y) - (\phi_\theta(x))_i (\phi_\theta(y))_i}} \\
			&\leq \sum_{i=1}^{d}{\paren{
				\abs{\sqrt{\lambda_i} \psi_i(y)} \abs{\sqrt{\lambda_i} \psi_i(y) - (\phi_\theta(y))_i}
				+ \abs{\sqrt{\lambda_i} \psi_i(y)} \abs{\sqrt{\lambda_i} \psi_i(x) - (\phi_\theta(x))_i}
				}} \\
			&\stepa{\leq} C(r) \sum_{i=1}^{d}{\paren{
				\abs{\sqrt{\lambda_i} \psi_i(y) - (\phi_\theta(y))_i}
				+ \abs{\sqrt{\lambda_i} \psi_i(x) - (\phi_\theta(x))_i}
				}} \\
			&\stepb{\leq} 2 C(r) \cdot d \cdot \epsilon_1,
		\end{split}
	\end{equation*}
	where step (a) is by the definition $C(r) \coloneqq \max_{x \in \calX} \lvert \sqrt{\lambda_i} \psi_i(x) \rvert$ (\Cref{def:sym_pd_ker_specturm_decay}) and step (b) is by~\Cref{eq:proof_NN_UAP}.

	Let the neural network approximation error be $\epsilon_1 = \frac{\epsilon}{4 C(r) d_r(\epsilon / 2)}$ such that the above is bounded by $\epsilon / 2$. 

	Then, by~\eqref{eq:proof_approx_bound}, we have that
	\begin{equation*}
		\sup_{x, y \in \mathcal{X}}{
			\abs{r(x,y) - \iprod{\phi_\theta(x)}{\phi_\theta(y)}}} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
	\end{equation*}

	Hence, the relation function $r$ is approximated uniformly on $\calX \times \calX$ by an inner product of neural networks with at most $d_r(\epsilon / 2) \cdot \calN_\calX\paren{\frac{\epsilon}{4 C(r) d_r(\epsilon / 2)}}$ neurons.

\end{proof}

\Cref{theorem:symmetric_inner_prod_rels_func_class} states that pairwise relations modeled as inner products of neural networks can capture any symmetric positive definite kernel. Moreover, the scale of neural network needed to achieve a particular approximation error is characterized in terms of the complexity of the kernel relation function and the efficiency of the neural network function class. In particular, this dependence is expressed in the kernel relation's spectrum decay $d_r(\cdot)$ and the neural network's efficiency $\calN_\calX(\cdot)$.

The efficiency of neural network's in approximating arbitrary functions has been studied extensively. Different results can give different bounds on $\calN_\calX(\cdot)$. The following result follows from a particular bound obtained in~\parencite{poggioWhyWhenCan2017}.

\begin{corollary}
	Consider the setting of~\Cref{theoorem:asymemtric_inner_prod_rel_func_class} with the neural network $\phi_\theta$ a shallow neural network with ReLU activations,
	\begin{equation*}
		\phi_\theta(x) = \sum_{k=1}^{n} a_k \sigma(\iprod{w_k}{x} + b_k),
	\end{equation*}
	where $a_k, b_k \in \reals$, $w_k \in \reals^{\mathrm{dim}(\calX)}$, and $\theta = (a_1, \ldots, a_n, b_1, \ldots, b_n, w_1, \ldots, w_n) \in \reals^{n (\mathrm{dim}(\calX) + 2)}$, and $n$ is the number of neurons. Then, there exists $\theta$ achieving $\sup_{x,y \in \mathcal{X}}{\abs{r(x,y) - \hat{r}(x,y)}} < \epsilon$ in~\Cref{theorem:symmetric_inner_prod_rels_func_class} with a number of neurons $n$ of the order 
	\[\calO\paren{d_r(\epsilon / 2) \cdot \paren{\frac{\epsilon}{4 C(r) d_r(\epsilon / 2)}}^{-\mathrm{dim}(\calX)}}.\]
\end{corollary}
\begin{proof}
	This follows by~\Cref{theorem:symmetric_inner_prod_rels_func_class} and~\parencite[Theorem 4]{poggioWhyWhenCan2017}.
\end{proof}

% TODO: double check that I'm interpreting . Sorting through the literature of universal approximation results is a bit of a challenge: they make different kinds of regularity assumptions (e.g., smoothness up to a certain order, etc) and sometimes have distribution-dependent rather than uniform bounds.

\begin{remark}
	The result also holds for universal approximators other than feedforward neural networks, provided that the same uniform approximation property holds.
\end{remark}