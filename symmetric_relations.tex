\section{Function class of symmetric inner product relations}\label{sec:symmetric_relations}

Consider a symmetric relation $r: \calX \times \calX \to \reals$ modeled as the inner product of neural network encodings of a pair of inputs, $x, y \in \calX$,
\begin{equation}\label{eq:symmetric_iprod_relation}
    r(x, y) = \iprod{\phi(x)}{\phi(y)},
\end{equation}
where $\phi$ is a neural network. Symmetric relation functions modeled in this way are natural \textit{measures of similarity}. Intuitively, $\phi$ extracts features of the objects $x,y$, and the inner product computes the similarity between those features. To see this more formally, suppose we have a normalized inner product relation (i.e., $\phi: \calX \to \bbS^{d}$), then we have
\begin{equation}
    \twonorm{\phi(x) - \phi(y)}^2 = 2 - 2 r(x, y).
\end{equation}

% The Cauchy-Schwartz inequality states that
% \begin{equation}
%     r(x, y)^2 \leq r(x, x) r(y, y) = 1, \ \forall x, y \in \calX.
% \end{equation}

Thus, $r(x,y)$ is large and close to $1$ when $x$ and $y$ are similar (with respect to $\phi$), and close to $0$ when $x, y$ are dissimilar. Moreover, $r: \calX \times \calX \to \reals$ induces a geometry on $\calX$ via the pseudometric $d(x,y) = \sqrt{2 - 2 r(x,y)}$. The triangle inequality of the pseudometric corresponds to a notion of transitivity---if $x$ is related to $y$ and $y$ is related to $z$, then $x$ must be related to $z$.

In this section, we characterize the class of relation functions that can be modeled by symmetric inner products of neural networks. We show that any continuous symmetric positive definite kernel (i.e., Mercer kernel) can be approximated by a symmetric inner product of neural networks. This characterization is strict since $r(x,y) = \iprod{\phi(x)}{\phi(y)}$ is a Mercer kernel for any $\phi$ because $\forall g \in L^2(\calX)$ we have,
\begin{equation*}
    \begin{split}
        \int \int g(x) r(x,y) g(y) dx dy &= \int \int g(x) \iprod{\phi(x)}{\phi(y)} g(y) dx dy \\
        &= \iprod{\int g(x) \phi(x) dx}{\int g(y) \phi(y) dy} \\
        &= \twonorm{\int g(x) \phi(x) dx}^2 \geq 0.
    \end{split}
\end{equation*}

The main result in this section states that if $\phi$ is a universal approximator such as a multi-layer perceptron, then for any positive-definite symmetric kernel $r$, there exists a neural network $\phi$ such that the induced inner product relation approximates $r$ arbitrarily well. To state the result, we begin with some preliminaries.

\textbf{Preliminaries: symmetric positive definite kernels.} Consider a symmetric positive definite kernel $K: \calX \times \calX \to \reals$ on a compact euclidean space $\calX$. By Mercer's theorem~\parencite{mercerFunctionsPositive1909, sunMercerTheorem2005, micchelliUniversalKernels2006}, there exist non-negative eigenvalues $\set{\lambda_i}_{i=1}^{\infty}$ and continuous eigenfunctions $\set{\psi_i}_{i=1}^{\infty}$ such that for any $x,y \in \calX$, $K(x, y) = \sum_{i=1}^\infty \lambda_i \psi_i(x) \psi_i(y)$, and the convergence of the series is absolute and uniform over $\calX$. The following definition characterizes, in a sense, the complexity of a symmetric positive-definite kernel in terms of its spectrum.

\begin{assumption}[Kernel spectrum decay]\label{ass:sym_pd_ker_specturm_decay}
	Let $K: \calX \times \calX \to \reals$ be a symmetric positive-definite kernel, with eigenvalues $\{\lambda_i\}_{i=1}^{\infty}$ and continuous eigenfunctions $\{\psi_i\}_{i=1}^{\infty}$. For $\epsilon > 0$, denote by $d_K(\epsilon) \in \naturals$ the minimum integer such that,
	\begin{equation*}
		\sup_{x, y \in \calX} \abs{K(x, y) - \sum_{i=1}^{d_K(\epsilon)} \lambda_i \psi_i(x) \psi_i(y)} \leq \epsilon.
	\end{equation*}
	Moreover, let $C(K, d) \coloneq \max_{i \in [d]} \max_{x \in \calX} \abs{\sqrt{\lambda_i} \psi_i(x)}$.% and let $L(K, d) \coloneq \max_{i \in [d]} L_i$, where $L_i$ is the Lipschitz constant of $\sqrt{\lambda_i} \psi_i$.
\end{assumption}

The quantity $d_K(\epsilon)$ associated with a kernel $K$ describes the number of eigenfunctions needed to approximate $K$ up to an error of $\epsilon$. For any symmetric positive-definite kernel $K$, $d_K(\epsilon)$ is finite for any $\epsilon > 0$. 
% Similarly, the quantity $L(K, d)$ describes the level of ``regularity'' of the first $d$ eigenfunctions of $r$. 
Note that $C(K, d)$ is finite for any $d$ since $\calX$ is assumed compact and the eigenfunctions $\psi_i$ are continuous. % Moreover, $d_K(\epsilon)$ increases as $\epsilon \to 0^+$, and $C(K, d), L(K, d)$ are increasing in $d$. Hence, $C(K, d_K(\epsilon)), L(K, d_K(\epsilon))$ is decreasing in $\epsilon$.

\textbf{Preliminaries: universal approximation of neural networks.} Let $\calV_N$ be a class of neural networks with ``complexity'' $N$. In this work, we take $N$ to be the total number of neurons. We assume that $\calV_{N+1} \supset \calV_N$, and denote $\calV = \cup_N{\calV_N}$. Let $\calF$ be a space of functions from $\calX$ to $\reals$ and let $\nnorm{\cdot}_\calF$ be a norm on functions from $\calX$ to $\reals$. For example, $\nnorm{\cdot}_\calF$ may be the sup-norm or an $L^p$ norm, and $\calF$ may be a class of functions of a particular smoothness. For multivariate functions from $\calX^n$ to $\reals$, we denote the corresponding norm by $\nnorm{\cdot}_{\calF^{\otimes n}}$. In the case of the sup-norm, it is $\sup_{\bm{x} \in \calX^n} \abs{f(x_1, \ldots, x_n)}$ and in the case of a $L^p$ norm it is $\pparen{\int_{\calX^n} \abs{f(x_1, \ldots, x_n)}^p d\mu^{\otimes n}(x_1, \ldots, x_n)}^{1/p}$. We will consider approximating the relation function $r: \calX \times \calX \to \reals$ with respect to $\nnorm{\cdot}_{\calF^{\otimes 2}}$.

Universal approximation results provide theoretical guarantees about how well a class of networks of a given complexity can approximate a given function with respect to $\nnorm{\cdot}_\calF$. In particular, for a given function $f$, universal approximation results characterize the quantity
\[\mathrm{dist}(f, \calV_N) = \inf_{g \in \calV_N} \norm{f - g}_{\calF}.\]
Alternatively, this can be stated as follows: for a desired approximation error $\epsilon$, how large does $N$ need to be so that there exists $g \in \calV_N$ such that $\norm{f - g}_{\calF} \leq \epsilon$? We state this below as an assumption on the network class $\calV$.

\begin{assumption}[Efficiency of function approximator]\label{ass:univ_approx_efficiency}
	Consider a class of neural networks $\calV = \cup_N \calV_N$ on a space $\calX$ where $N$ is a scalable complexity. Let $\calF$ be a class of functions on $\calX$. For $\epsilon > 0$, denote by $\calN_\calF(\epsilon) \in \naturals$ the minimum integer such that for any function $f \in \calF$, there exists a neural network $g \in \calV_N$ with $N = \calN_\calF(\epsilon)$ that approximates $f$ with respect to $\norm{\cdot}_\calF$ with error bounded by $\epsilon$. That is,
	\begin{equation*}
		\sup_{f \in \calF} \, \inf_{g \in \calV_N} \norm{f - g}_{\calF} \leq \epsilon.
	\end{equation*}
	We assume a universal approximation property on the class of neural networks $\calV$: for every $\epsilon > 0$, $\calN_\calF(\epsilon)$ is finite.
\end{assumption}

We are now ready to formally state the main result of this section.

\begin{theorem}[Function class of symmetric inner product relational neural networks]\label{theorem:symmetric_inner_prod_rels_func_class}
	Suppose the data lies in a compact Euclidean space $\mathcal{X}$. Let $\calF$ be a function space from $\calX$ to $\reals$, and $\nnorm{\cdot}_\calF$ a norm on functions from  $\calX$ to $\reals$.
	Let $r: \calX \times \calX$ be a symmetric positive-definite kernel satisfying~\Cref{ass:sym_pd_ker_specturm_decay} and suppose $\sqrt{\lambda_i} \psi_i \in \calF, i \in [d_r(\epsilon / 2)]$, where $\lambda_i, \psi_i$ are the eigenvalues and eigenfunctions of $r$.
	Let $\calV = \sset{\calV_N}_N$ be a family of neural networks satisfying~\Cref{ass:univ_approx_efficiency}.
	Then for any $\epsilon > 0$, there exists a neural network $\phi \in \calV_N$ such that 
	\[\norm{r(x,y) - \iprod{\phi(x)}{\phi(y)}}_{\calF^{\otimes 2}} < \epsilon.\]
	Moreover, the complexity of the neural network required is bounded by
	\[d_r(\epsilon/2) \cdot \calN_\calF\paren{\frac{\epsilon}{4 C(r) d_r(\epsilon/2)}}.\]
\end{theorem}

\begin{proof}
	By Mercer's theorem~\parencite{mercerFunctionsPositive1909, sunMercerTheorem2005, micchelliUniversalKernels2006}, there exists $(\psi_i)_{i \in \mathbb{N}}$, $\lambda_i \geq 0$ such that $r(x,y) = \sum_{i=1}^{\infty}{\lambda_i \psi_i(x) \psi_i(y)}$, where $\psi_i$ and $\lambda_i$ are eigenfunctions and eigenvalues of the integral operator $T_r: L^2(\mathcal{X}) \to L^2(\mathcal{X}),\,T_r: f \mapsto \int_{\mathcal{X}}{r(\cdot, x) f(x) dx}$.
	Furthermore, the convergence of the series is uniform:
	\begin{equation}
		\lim_{n \to \infty} \sup_{x,y \in \mathcal{X}} \lvert r(x,y) - \sum_{i=1}^{n}{\lambda_i \psi_i(x) \psi_i(y) \rvert} = 0.
	\end{equation}
	Using the notation of~\Cref{ass:sym_pd_ker_specturm_decay}, we have that
	\begin{equation}\label{eq:proof_mercer_thm_unif_abs_cv}
		\sup_{x,y \in \mathcal{X}} \left\lvert r(x,y) - \sum_{i=1}^{d_r(\epsilon/2)}{\lambda_i \psi_i(x) \psi_i(y)} \right\rvert < \frac{\epsilon}{2},
	\end{equation}
	where $d_r(\epsilon/2)$ is defined as in~\Cref{ass:sym_pd_ker_specturm_decay}.

	Let $d \coloneqq d_r(\epsilon / 2)$ be the output dimension of the neural network $\phi$ such that it maps from $\calX$ to $\reals^d$. Our approach will be to approximate with the MLP $\phi$ the first $d$ eigenfunctions of the kernel relation $r$, $(\sqrt{\lambda_1} \psi_1, \ldots, \sqrt{\lambda_{d}} \psi_{d})$.
	% The function we would like to approximate with $\phi$ is $(\sqrt{\lambda_1} \psi_1, \ldots, \sqrt{\lambda_{d}} \psi_{d})$, the first $d$ eigenfunctions of the kernel relation $r$.
	By the universal approximation property of $\calV$ (\Cref{ass:univ_approx_efficiency}), for any $\epsilon_1 > 0$, there exists a neural network $\phi = \pparen{\phi_1, \ldots, \phi_d} \in \calV_N$ for some $N$ such that
	\begin{equation}\label{eq:proof_NN_UAP}
		\norm{\phi_i - \sqrt{\lambda_i} \psi_i}_\calF < \epsilon_1, \ \forall i \in \{1, \ldots, d\},
	\end{equation}
	where $\phi_i(x)$ is the $i$-th component of $\phi(x)$. Moreover, by~\Cref{ass:univ_approx_efficiency} there exists such a $\phi$ with a number of neurons bounded by $d \cdot \calN_\calF(\epsilon_1)$.

	Now note that the approximation error for $r$ is bounded by
	\begin{equation}\label{eq:proof_approx_bound}
		\begin{split}
			&\bignorm{r(x,y) - \langle \phi_\theta(x), \phi_\theta(y) \rangle}_{\calF^{\otimes 2}}\\
			&= \norm{ r(x,y) - \sum_{i=1}^{d}{\phi_i(x) \phi_i(y)}}_{\calF^{\otimes 2}} \\
			&\leq \norm{ r(x,y) - \sum_{i=1}^{d}{\lambda_i \psi_i(x) \psi_i(y)}}_{\calF^{\otimes 2}} + \norm{\sum_{i=1}^{d}\lambda_i \psi_i(x) \psi_i(y) - \sum_{i=1}^{d}\phi_i(x) \phi_i(y)}_{\calF^{\otimes 2}}
		\end{split}
	\end{equation}
	The first term is bounded by $\frac{\epsilon}{2}$ by~\eqref{eq:proof_mercer_thm_unif_abs_cv}. The second term can be bounded by noting the for any $x, y \in \calX$
	\begin{equation*}
		\begin{split}
			&\abs{\sum_{i=1}^{d}\lambda_i \psi_i(x) \psi_i(y) - \sum_{i=1}^{d}\phi_i(x) \phi_i(y)} \\
			&\leq \sum_{i=1}^{d}{ \abs{ \lambda_i \psi_i(x) \psi_i(y) - \phi_i(x) \phi_i(y)}} \\
			&\leq \sum_{i=1}^{d}{\paren{
				\abs{\sqrt{\lambda_i} \psi_i(y)} \abs{\sqrt{\lambda_i} \psi_i(y) - \phi_i(y)}
				+ \abs{\sqrt{\lambda_i} \psi_i(y)} \abs{\sqrt{\lambda_i} \psi_i(x) - \phi_i(x)}
				}} \\
			&\leq C(r) \sum_{i=1}^{d}{\paren{
				\abs{\sqrt{\lambda_i} \psi_i(y) - \phi_i(y)}
				+ \abs{\sqrt{\lambda_i} \psi_i(x) - \phi_i(x)}
				}} \\
		\end{split}
	\end{equation*}
	where the last line is by the definition $C(r) \coloneqq \max_{x \in \calX} \lvert \sqrt{\lambda_i} \psi_i(x) \rvert$ (\Cref{ass:sym_pd_ker_specturm_decay}). Now, by~\Cref{eq:proof_NN_UAP}, we have
	\begin{align*}
		&\norm{\sum_{i=1}^{d}\lambda_i \psi_i(x) \psi_i(y) - \sum_{i=1}^{d}\phi_i(x) \phi_i(y)}_{\calF^{\otimes 2}} \\
		&\leq \norm{C(r) \sum_{i=1}^{d}{\paren{\abs{\sqrt{\lambda_i} \psi_i(y) - \phi_i(y)} + \abs{\sqrt{\lambda_i} \psi_i(x) - \phi_i(x)}}}}_{\calF^{\otimes 2}}\\
		&\leq C(r) \sum_{i=1}^{d}\paren{\norm{\sqrt{\lambda_i} \psi_i(y) - \phi_i(y)}_{\calF^{\otimes 2}} + \norm{\sqrt{\lambda_i} \psi_i(x) - \phi_i(x)}_{\calF^{\otimes 2}}}\\
		&\leq 2 C(r) \cdot d \cdot \epsilon_1,
	\end{align*}

	Let the neural network approximation error be $\epsilon_1 = \frac{\epsilon}{4 C(r) d_r(\epsilon / 2)}$ such that the above is bounded by $\epsilon / 2$. 

	Then, by~\eqref{eq:proof_approx_bound}, we have that
	\begin{equation*}
			\bignorm{r(x,y) - \iprod{\phi(x)}{\phi(y)}}_{\calF^{\otimes 2}} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
	\end{equation*}

	Hence, the relation function $r$ is approximated with respect to $\nnorm{\cdot}_{\calF^{\otimes 2}}$ by an inner product of neural networks with number of neurons at most 
	\[d_r(\epsilon / 2) \cdot \calN_\calF\paren{\frac{\epsilon}{4 C(r) d_r(\epsilon / 2)}}.\]

\end{proof}

\Cref{theorem:symmetric_inner_prod_rels_func_class} states that pairwise relations modeled as inner products of neural networks can capture any symmetric positive definite kernel. Moreover, the scale of the neural network needed to achieve a particular approximation error is characterized in terms of the complexity of the kernel relation function and the efficiency of the neural network function class. In particular, this dependence is expressed in the kernel relation's spectrum decay $d_r(\cdot)$ and the neural network's efficiency $\calN_\calF(\cdot)$.

\begin{remark}
	In the results above, the number of neurons is bounded by ``$d_r \cdot \calN$''. This is an upper bound assuming each of the $d_r$ kernel eigenfunctions is modeled independently. In practice, the size of the neural network needed would be smaller since the eigenfunctions can be approximated with a single MLP with a $d_r$-dimensional output, allowing for computations to be re-used across several output dimensions.
\end{remark}

The efficiency of neural networks in approximating arbitrary functions has been studied extensively. Different results in the literature characterize $\calN_{\calF}(\cdot)$ as a function of the class of neural networks (e.g., shallow or deep) and the function class $\calF$ (e.g., degree of smoothness). In the following corollary, we specialize~\Cref{theorem:symmetric_inner_prod_rels_func_class} to approximation with respect to the sup-norm $\nnorm{\cdot}_\calF = \nnorm{\cdot}_\infty$ and $\calF$ the class of Lipschitz functions.

\begin{corollary}\label{cor:sym_iprod_kernel_neuron_bound}
	Consider the setting of~\Cref{theorem:symmetric_inner_prod_rels_func_class}. Suppose further that $\calX = [-1, 1]^d$ and let $L(r, k) \coloneq \max_{i \in [k]} L_i$, where $L_i$ is the Lipschitz constant of $\sqrt{\lambda_i} \psi_i$ and $\lambda_i, \psi_i$ are the eigenvalues and eigenfunctions of the symmetric relation function $r$. Consider $\calV$ to be the class of shallow neural networks with ReLU activations with $\phi \in \calV$ given by
	\begin{equation*}
		\phi(x) = \sum_{k=1}^{N} a_k \mathrm{ReLU}(\iprod{w_k}{x} + b_k),
	\end{equation*}
	where $a_k, b_k \in \reals$, $w_k \in \reals^{d}$ are the parameters, and $N$ is the number of neurons. Then, for any $\epsilon > 0$, there exists $\phi \in \calV_N$ achieving $\sup_{x,y \in \mathcal{X}}{\abs{r(x,y) - \iprod{\phi(x)}{\phi(y)}}} < \epsilon$ with a number of neurons $N$ of the order
	\[\calO\paren{d_r(\epsilon / 2) \cdot \paren{\frac{4 C(r) d_r(\epsilon / 2) L(r, d_r(\epsilon/2))}{\epsilon}}^{d}}.\]
\end{corollary}
The proof of this result follows from~\Cref{theorem:symmetric_inner_prod_rels_func_class} and~\textcite{bachBreakingCurseDimensionality2016}.

%\aanote[margin, noinline]{this discussion is new}
We observe a curse of dimensionality in this approximation result, where the number of neurons needed to achieve a good approximation grows exponentially in the dimension $d$ of the underlying space $\calX$. This phenomenon has been studied in the context of neural network approximation results, and it's well knwon that imposing further structural assumptions on the functions to be approximated can reduce or remove the dependence on dimension. One approach to obtaining a more favorable dependence on the dimension was outlined by~\textcite{barronUniversalApproximation1993}'s seminal work, which identifies a smoothness condition based on the Fourier representation of the underlying function.  The \textit{Barron norm} of a function $f: \reals^d \to \reals$ is defined as
\[\nnorm{f}_{\calB} \coloneq \int_{\reals^d} \nnorm{\hat{\nabla f}(\omega)} d\omega = \int_{\reals^d} \nnorm{\omega} \aabs{\hat{f}(\omega)} d \omega,\]
where $\hat{f}$ denotes the Fourier transform. The advantage of this measure of smoothness is that, for some interesting classes of functions, it scales sub-exponentially with the dimension. In the following corollary, we show that a more favorable dependence on the dimension can be obtained if the eigenfunctions of the relation are smooth in the sense of the Barron norm.

\begin{corollary}\label{cor:sym_iprod_kernel_barron_neuron_bound}
<<<<<<< HEAD
	Consider the setting of~\Cref{theorem:symmetric_inner_prod_rels_func_class}. Suppose that $\calX$ has radius $\mathrm{radius}(\calX)$, so that $\nnorm{x} \leq \mathrm{radius}(\calX),\, \forall x \in \calX$. Assume that $\nnorm{\sqrt{\lambda_i} \psi_i} \leq B(r)$ for $i \in [d_r(\epsilon/2)]$. Consider $\calV$ to be the class of shallow neural networks with sigmoid activations with $\phi \in \calV$ given by
=======
	Consider the setting of~\Cref{theorem:symmetric_inner_prod_rels_func_class}. Suppose that $\calX$ has radius $\mathrm{radius}(\calX)$ (i.e., $\nnorm{x} \leq \mathrm{radius}(\calX),\, \forall x \in \calX$). Assume that $\nnorm{\sqrt{\lambda_i} \psi_i}_{\calB} \leq B(r)$ for $i \in [d_r(\epsilon/2)]$. Consider $\calV$ to be the class of shallow neural networks with sigmoid activations with $\phi \in \calV$ given by
>>>>>>> 8e1150cfcda463a73599492974134e1c2fd29c9b
	\begin{equation*}
		\phi(x) = \sum_{k=1}^{N} a_k \sigma(\iprod{w_k}{x} + b_k),
	\end{equation*}
	where $a_k, b_k \in \reals$, $w_k \in \reals^{d}$ are the parameters, and $N$ is the number of neurons. Then, for any $\epsilon > 0$, there exists $\phi \in \calV_N$ achieving $\nnorm{r(x,y) - \iprod{\phi(x)}{\phi(y)}}_{L^2} < \epsilon$ with a number of neurons $N$ of the order
	\[\calO\paren{d_r(\epsilon / 2)^3 \cdot \frac{\paren{C(r) \cdot \mathrm{radius}(\calX)\, \cdot B(r)}^2}{\epsilon^2}}.\]
\end{corollary}
<<<<<<< HEAD
The proof of this statement follows from~\Cref{theorem:symmetric_inner_prod_rels_func_class} and~\textcite{barronUniversalApproximation1993}.

=======
\begin{proof}
	This follows by~\Cref{theorem:symmetric_inner_prod_rels_func_class} and~\textcite{barronUniversalApproximation1993}.
\end{proof}

This result shows that the size of the neural network needed to approximate $r$ scales with its spectrum decay $d_r(\cdot)$, which can be interpreted as a measure of the dimensionality or ``rank'' of the relation, and the smoothness of its eigenfunctions $B(r)$.
>>>>>>> 8e1150cfcda463a73599492974134e1c2fd29c9b
