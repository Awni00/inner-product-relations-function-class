\section{Discussion}

The analysis in this note underscores the importance of kernels for learning relations and attention mechanisms. In the symmetric case, the assumption of a positive-definite kernel function is natural, leading to the standard framework of reproducing kernel Hilbert spaces. In the asymmetric case, which is arguably more important and applicable for relational learning, a different technical approach is needed, and reproducing kernel Banach spaces arises naturally. After completing the work presented here, we became aware of the related work of \citet{wright2021transformers}, which makes this connection as well.

The results presented here can be extended in several ways. For example, the bounds on the number of neurons in a perceptron that suffice to approximate a relation function to a given accuracy can likely be sharpened, drawing on the extensive literature on approximation properties of neural networks \citep[e.g.,][]{petrushev1998approximation,pinkus1999approximation,makovoz1998uniform,burger2001error,maiorov2006approximation,bachBreakingCurseDimensionality2016}. In terms of attention mechanisms in transformers, our initial focus was on approximating the most relevant key to a given query. The representation theorem of \citet{debreuRepresentationPreferenceOrdering1954} is used to express the problem in terms of a utility function, which is then approximated. It would be of interest to derive approximation bounds for the full distribution of attention values that are computed by the softmax function in Transformers. Finally, when considering relational learning, the possibility of higher-order, recursive relations, naturally arises \citep[e.g.,][]{altabaaRelationalConvolutionalNetworks2023}, and it may be interesting to study function spaces of hierarchical relations in such settings.
