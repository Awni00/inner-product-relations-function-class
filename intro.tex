\section{Introduction}\label{sec:intro}

Machine learning systems must be able to represent and reason about relations between objects, either explicitly or implicitly. For example, a natural language understanding system takes a sequence of words as input and extracts information about the meaning of the words based on relations between them in the local context. Similarly, a scene analysis system considers the relations between the components of a scene in order to identify and interpret the objects. 

A common way to represent relations between objects is through inner products between feature representations of the form $\iprod{\phi(x)}{\psi(y)}$, where $x, y \in \calX$ are two objects and $\phi, \psi$ are neural network feature maps. The properties of inner products often make them appropriate measures of similarity. The aim of this paper is to understand the representational power of this model by characterizing the class of relation functions $r: \calX \times \calX \to \reals$ which can be represented as inner products of neural networks.

Inner products between neural network transformations of objects appear in many machine learning architectures. A notable example is the attention mechanisms that lie at the heart of many sequence models, including the Transformer~\parencite{vaswani2017attention}. In the Transformer, self-attention is implemented as
\begin{equation*}
    \begin{split}
        \alpha_{ij} &\gets \mathrm{Softmax}_j\paren{{\iprod{\phi_q(x_i)}{\phi_k(x_j)}}}\\
        x_i' &\gets \sum_{j=1}^{n} \alpha_{ij} \phi_v(x_j)
    \end{split}
\end{equation*}
where $\phi_q, \phi_k$, and $\phi_v$ are learned transformations and $\iprod{\phi_q(x_i)}{\phi_k(x_j)}$ represents a relation between $x_i$ and $x_j$, which determines how much $i$ should attend to $j$.

The Transformer models relations implicitly through its attention mechanism. An example of an explicitly relational model using inner products to represent relations is CoRelNet~\parencite{kergNeuralArchitecture2022}, where a similarity matrix is computed consisting of symmetric inner products between each pair of objects, $\bra{R_{i,\cdot}}_{i=1}^m = \mathrm{Softmax}_j\paren{{\iprod{\phi(x_i)}{\phi(x_j)}}}$.

Siamese networks are another domain where understanding the function classes of inner products of neural network transformations is relevant~\parencite[e.g.,][]{chopraLearningSimilarityMetric2005}. There, the relevant quantity is a distance between transformations of two objects. If the distance is the Euclidean distance, then $\twonorm{\phi(x) - \phi(y)}^2 = \twonorm{\phi(x)}^2 + \twonorm{\phi(y)}^2 - 2 \iprod{\phi(x)}{\phi(y)}$, where again the inner product of neural networks arises.

In this paper we characterize the function class of inner products of neural networks, showing that inner products of neural networks are universal approximators for relation functions. In particular, when the inner product of neural networks is symmetric (i.e., $\phi=\psi$), the function class is the class of continuous positive definite kernels. When the inner product is asymmetric (i.e., $\phi \neq \psi$), the function class is the class of continuous functions on $\calX \times \calX$. The symmetric case is related to kernels of reproducing kernel Hilbert spaces whereas the asymmetric case admits a connection to kernels of reproducing kernel Banach spaces.