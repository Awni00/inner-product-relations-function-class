\section{Introduction}\label{sec:intro}

\aanote[margin, noinline]{update abstract?}

Machine learning systems must be able to represent and reason about relations between objects, either explicitly or implicitly. For example, a natural language understanding system takes a sequence of words as input and extracts information about the meaning of the words based on relations between them in the local context. Similarly, a scene analysis system considers the relations between the components of a scene in order to identify and interpret the objects. 

A common way to represent relations between objects is through inner products between feature representations of the form $\iprod{\phi(x)}{\psi(y)}$, where $x, y \in \calX$ are two objects and $\phi, \psi$ are neural network feature maps. Inner products posses properties which make them useful measures of similarity. The aim of this paper is to understand the representational power of this model by characterizing the class of relation functions $r: \calX \times \calX \to \reals$ which can be represented as inner products of neural networks.

Inner products between neural network feature maps of objects appear in many machine learning architectures. A notable example is the attention mechanisms that lie at the heart of sequence models like the Transformer~\parencite{vaswani2017attention}. In the Transformer, self-attention is implemented as
\begin{equation*}
    \begin{split}
        \alpha_{ij} &\gets \mathrm{Softmax}\paren{\bra{{\iprod{\phi_q(x_i)}{\phi_k(x_j)}}}_{j \in [n]}}_j\\
        x_i' &\gets \sum_{j=1}^{n} \alpha_{ij} \phi_v(x_j)
    \end{split}
\end{equation*}
where $\phi_q, \phi_k$, and $\phi_v$ are learned transformations and $\iprod{\phi_q(x_i)}{\phi_k(x_j)}$ represents a relation between $x_i$ and $x_j$, which determines how much $i$ should attend to $j$.

The Transformer models relations implicitly through its attention mechanism. Modeling relations through inner products of features is also central to many ``explicitly relational'' neural architectures~\parencite[e.g.,][]{webbEmergentSymbols2021,kergNeuralArchitecture2022,altabaaAbstractorsTransformer2023,altabaaRelationalConvolutionalNetworks2023}. For example, in the CoRelNet model~\parencite{kergNeuralArchitecture2022}, a similarity matrix is computed consisting of symmetric inner products between each pair of objects, $R_{i,\cdot} = \mathrm{Softmax}\pparen{\bra{\iprod{\phi(x_i)}{\phi(x_j)}}_{j\in[n]}}$.~\cite{altabaaRelationalConvolutionalNetworks2023} propose a Transformer-based architecture imbued with relational inductive biases by replacing the values $\phi_v(x_i)$ with vector representations which identify objects but are independent of object-level features.~\cite{altabaaRelationalConvolutionalNetworks2023} propose a relational architecture where the central operation is a type of ``convolution'' operating on a tensor of relations computed via inner products of feature maps.

\aanote[margin, noinline]{what other work can we mention that uses inner products to model some kind of relation or similarity?}{are we talking too much about our own work? what other work can we mention that uses inner products to model some kind of relation or similarity? talk more about attention?}

\aawarning[pdfcnote, noinline]{These two paragraphs are talking about architectures which involve inner products of neural network feature maps as a means to model relations? How do we want to organize this? a separate ``related work'' section or just right here? what other architectures do we want to mention? any other relevant well-known work? can mention relational Relational recurrent neural networks}

Siamese networks are another domain where understanding the function classes of inner products of neural network transformations is relevant~\parencite[e.g.,][]{chopraLearningSimilarityMetric2005}. There, the relevant quantity is a distance between transformations of two objects. If the distance is the Euclidean distance, then $\twonorm{\phi(x) - \phi(y)}^2 = \twonorm{\phi(x)}^2 + \twonorm{\phi(y)}^2 - 2 \iprod{\phi(x)}{\phi(y)}$, where again the inner product of neural networks arises.

\aawarning[margin, noinline]{[todo]: expand a bit on siamese networks. what were they designed to do? compare things...}

% universal approximation of feedforward neural networks has been extensively studied (e.g., classical references are ... Barron, Cybenko, etc.)
In this paper we characterize the function class of inner products of neural networks, showing that inner products of neural networks are universal approximators for relation functions. In particular, when the inner product of neural networks is symmetric (i.e., $\phi=\psi$), the function class is the class of continuous positive definite kernels. When the inner product is asymmetric (i.e., $\phi \neq \psi$), the function class is all continuous bivariate functions on $\calX \times \calX$. The symmetric case is related to kernels of reproducing kernel Hilbert spaces whereas the asymmetric case admits a connection to kernels of reproducing kernel Banach spaces. We apply these results to analyzing the attention operation, showing that any retrieval function described by arbitrary pre-order define 