\section{Introduction}\label{sec:intro}

Modeling relations between a set of objects is a fundamental problem in cognition and machine learning. For example, a natural language understanding system takes a sequence of words as input and must reason about the meaning of the words as well as the relations between them. Similarly, a visual reasoning system, once it has identified objects in a scene, must reason about the relations between those objects. Machine learning systems applied to such tasks must be able to represent and reason about relations between objects, whether explicitly or implicitly.

A common way to represent relations between objects is through inner products between feature representations of the form $\iprod{\phi(x)}{\psi(y)}$, where $x, y \in \calX$ are two objects and $\phi, \psi$ are neural network feature maps. This is a natural approach since the properties of inner products make them good measures of similarity. The aim of this paper is to understand the representational power of this model by characterizing the class of relation functions $r: \calX \times \calX \to \reals$ which can be represented as inner products of neural networks.

Inner products between neural network transformations of different objects appear in several machine learning architectures. A notable example is in \textit{attention}. Attention mechanisms are at the heart of many sequence models, including the Transformer~\parencite{vaswani2017attention}. In the Transformer, self-attention is given by,
\begin{equation*}
    \begin{split}
        \alpha_{ij} &\gets \mathrm{Softmax}\paren{\bra{\iprod{\phi_q(x_i)}{\phi_k(x_j)}}_j}\\
        x_i' &\gets \sum_{j=1}^{n} \alpha_{ij} \phi_v(x_j)
    \end{split}
\end{equation*}
where $\phi_q, \phi_k$ are learned transformations and $\iprod{\phi_q(x_i)}{\phi_k(x_j)}$ represents some relation between $x_i$ and $x_j$ which determines how much $i$ should attend to $j$.

The Transformer models relations implicitly through its attention mechanism. An example of an explicitly relational model which uses inner products to represent relations is CoRelNet~\parencite{kergNeuralArchitecture2022}, where a similarity matrix is computed consisting of (symmetric) inner products between each pair of objects, $\bra{R_{i,\cdot}}_{i=1}^m = \mathrm{Softmax}\paren{\bra{\iprod{\phi(x_i)}{\phi(x_j)}}_j}$.

Siamese networks are another domain where understanding the function classes of inner products of neural network transformations is relevant~\parencite[e.g.,][]{chopraLearningSimilarityMetric2005}. There, the relevant quantity is a distance between transformations of two objects. If the distance is the euclidean distance, then $\twonorm{\phi(x) - \phi(y)}^2 = \twonorm{\phi(x)}^2 + \twonorm{\phi(y)}^2 - 2 \iprod{\phi(x)}{\phi(y)}$, where again the inner product of neural networks is relevant.

In this paper we characterize the function class of inner products of neural networks. We show that inner products of neural networks are universal approximations for relation functions. In particular, when the inner product of neural networks is symmetric (i.e., $\phi=\psi$), the function class is the class of continuous positive definite kernels. When the inner product is asymmetric (i.e., $\phi \neq \psi$), the function class is the class of continuous functions on $\calX \times \calX$. The symmetric case admits a connection to kernels of reproducing kernel Hilbert spaces whereas the asymmetric case admits a connection to kernels of reproducing kernel Banach spaces.