@article{altabaaAbstractorsTransformer2023,
  title = {Abstractors: {{Transformer Modules}} for {{Symbolic Message Passing}} and {{Relational Reasoning}}},
  shorttitle = {Abstractors},
  author = {Altabaa, Awni and Webb, Taylor and Cohen, Jonathan and Lafferty, John},
  date = {2023-04-01},
  url = {https://arxiv.org/abs/2304.00195v2},
  urldate = {2023-06-28},
  langid = {english},
  pubstate = {preprint}
}

@inproceedings{altabaaAbstractorsRelationalCrossattention2024,
  title = {Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers},
  shorttitle = {Abstractors},
  author = {Altabaa, Awni and Webb, Taylor and Cohen, Jonathan and Lafferty, John},
  booktitle = {12th International Conference on Learning Representations},
  year = {2024}
}


@article{barronUniversalApproximation1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A.R.},
  date = {1993-05},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.256500},
  url = {https://ieeexplore.ieee.org/document/256500/},
  urldate = {2023-03-31}
}

@article{cybenkoApproximationSuperpositions1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12},
  journaltitle = {Mathematics of Control, Signals, and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  url = {http://link.springer.com/10.1007/BF02551274},
  urldate = {2023-03-31},
  langid = {english}
}

@article{devitoExtensionMercer2011,
  title = {An Extension of {{Mercer}} Theorem to Vector-Valued Measurable Kernels},
  author = {De Vito, Ernesto and Umanita`, Veronica and Villa, Silvia},
  date = {2011-10-18},
  eprint = {1110.4017},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1110.4017},
  urldate = {2023-05-06},
  pubstate = {preprint}
}

@article{hornikMultilayerFeedforward1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2022-10-07},
  langid = {english}
}

@article{kergNeuralArchitecture2022,
  title = {On {{Neural Architecture Inductive Biases}} for {{Relational Tasks}}},
  author = {Kerg, Giancarlo and Mittal, Sarthak and Rolnick, David and Bengio, Yoshua and Richards, Blake and Lajoie, Guillaume},
  date = {2022-06-09},
  eprint = {2206.05056},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.05056},
  url = {http://arxiv.org/abs/2206.05056},
  urldate = {2022-09-13},
  pubstate = {preprint}
}

@article{mercerFunctionsPositive1909,
  title = {Functions of {{Positive}} and {{Negative Type}}, and Their {{Connection}} with the {{Theory}} of {{Integral Equations}}},
  author = {Mercer, J.},
  date = {1909},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series A},
  volume = {209},
  eprint = {91043},
  eprinttype = {jstor},
  pages = {415--446},
  publisher = {{The Royal Society}},
  issn = {0264-3952},
  url = {https://www.jstor.org/stable/91043},
  urldate = {2023-03-31}
}

@article{micchelliUniversalKernels2006,
  title = {Universal {{Kernels}}},
  author = {Micchelli, Charles A. and Xu, Yuesheng and Zhang, Haizhang},
  date = {2006},
  journaltitle = {Journal of Machine Learning Research},
  volume = {7},
  number = {95},
  pages = {2651--2667},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v7/micchelli06a.html},
  urldate = {2023-03-31}
}

@inproceedings{santoroRelationalRecurrent2018,
  title = {Relational Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/e2eabaf96372e20a9e3d4b5f83723a61-Abstract.html},
  urldate = {2022-11-11}
}

@article{santoroSimpleNeural2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  date = {2017-06-05},
  eprint = {1706.01427},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.01427},
  urldate = {2022-11-11},
  pubstate = {preprint}
}

@article{seelyNonSymmetricKernels1919,
  title = {Non-{{Symmetric Kernels}} of {{Positive Type}}},
  author = {Seely, Caroline E.},
  date = {1919-03},
  journaltitle = {The Annals of Mathematics},
  shortjournal = {The Annals of Mathematics},
  volume = {20},
  number = {3},
  eprint = {1967866},
  eprinttype = {jstor},
  pages = {172--176},
  issn = {0003486X},
  doi = {10.2307/1967866},
  url = {https://www.jstor.org/stable/1967866?origin=crossref},
  urldate = {2023-04-13}
}

@article{sunMercerTheorem2005,
  title = {Mercer Theorem for {{RKHS}} on Noncompact Sets},
  author = {Sun, Hongwei},
  date = {2005-06},
  journaltitle = {Journal of Complexity},
  shortjournal = {Journal of Complexity},
  volume = {21},
  number = {3},
  pages = {337--349},
  issn = {0885064X},
  doi = {10.1016/j.jco.2004.09.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X04000822},
  urldate = {2023-03-31},
  langid = {english}
}

@article{aronszajn1950theory,
  title={Theory of reproducing kernels},
  author={Aronszajn, Nachman},
  journal={Transactions of the American mathematical society},
  volume={68},
  number={3},
  pages={337--404},
  year={1950}
}

@article{webbEmergentSymbols2021,
  title = {Emergent {{Symbols}} through {{Binding}} in {{External Memory}}},
  author = {Webb, Taylor W. and Sinha, Ishan and Cohen, Jonathan D.},
  date = {2021-03-09},
  eprint = {2012.14601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.14601},
  url = {http://arxiv.org/abs/2012.14601},
  urldate = {2022-09-13},
  pubstate = {preprint}
}

@inproceedings{zhangReproducingKernel2009,
  title = {Reproducing Kernel {{Banach}} Spaces for Machine Learning},
  booktitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Zhang, Haizhang and Xu, Yuesheng and Zhang, Jun},
  date = {2009-06},
  pages = {3520--3527},
  publisher = {{IEEE}},
  location = {{Atlanta, Ga, USA}},
  doi = {10.1109/IJCNN.2009.5179093},
  url = {http://ieeexplore.ieee.org/document/5179093/},
  urldate = {2023-05-06},
  eventtitle = {2009 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2009 - {{Atlanta}})},
  isbn = {978-1-4244-3548-7},
  langid = {english}
}

@inproceedings{chopraLearningSimilarityMetric2005,
  author={Chopra, S. and Hadsell, R. and LeCun, Y.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Learning a similarity metric discriminatively, with application to face verification}, 
  year={2005},
  pages={539-546},
  doi={10.1109/CVPR.2005.202}}

@article{seelyNonSymmetricKernelsPositive1919,
  title = {Non-{{Symmetric Kernels}} of {{Positive Type}}},
  author = {Seely, Dr. Caroline E.},
  year = {1919},
  month = mar,
  journal = {The Annals of Mathematics},
  volume = {20},
  number = {3},
  eprint = {1967866},
  eprinttype = {jstor},
  pages = {172},
  issn = {0003486X},
  doi = {10.2307/1967866},
  urldate = {2023-04-13}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mhaskarDeepVsShallow2016,
  title = {Deep vs. Shallow Networks : {{An}} Approximation Theory Perspective},
  shorttitle = {Deep vs. Shallow Networks},
  author = {Mhaskar, Hrushikesh and Poggio, Tomaso},
  year = {2016},
  month = aug,
  urldate = {2024-01-03},
  howpublished = {https://arxiv.org/abs/1608.03287v1},
  langid = {english}
}

@article{okunoProbabilisticFrameworkMultiview2018,
  title = {A Probabilistic Framework for Multi-View Feature Learning with Many-to-Many Associations via Neural Networks},
  author = {Okuno, Akifumi and Hada, Tetsuya and Shimodaira, Hidetoshi},
  year = {2018},
  month = jun,
  number = {arXiv:1802.04630},
  eprint = {1802.04630},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2024-01-01},
  abstract = {A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is proposed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a probabilistic model for predicting new associations via graph embedding of the nodes of data vectors with links of their associations. Multi-view data vectors with many-to-many associations are transformed by neural networks to feature vectors in a shared space, and the probability of new association between two data vectors is modeled by the inner product of their feature vectors. While existing multi-view feature learning techniques can treat only either of many-to-many association or non-linear transformation, PMvGE can treat both simultaneously. By combining Mercer's theorem and the universal approximation theorem, we prove that PMvGE learns a wide class of similarity measures across views. Our likelihood-based estimator enables efficient computation of non-linear transformations of data vectors in large-scale datasets by minibatch SGD, and numerical experiments illustrate that PMvGE outperforms existing multi-view methods.},
  archiveprefix = {arxiv}
}

@article{poggioWhyWhenCan2017,
  title = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality: {{A}} Review},
  shorttitle = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  year = {2017},
  month = oct,
  journal = {International Journal of Automation and Computing},
  volume = {14},
  number = {5},
  pages = {503--519},
  issn = {1476-8186, 1751-8520},
  doi = {10.1007/s11633-017-1054-2},
  urldate = {2024-01-03},
  abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
  langid = {english}
}

@article{debreuRepresentationPreferenceOrdering1954,
  title = {Representation of a Preference Ordering by a Numerical Function},
  author = {Debreu, Gerard},
  year = {1954},
  journal = {Decision processes},
  volume = {3},
  pages = {159--165},
  publisher = {{Wiley New York}}
}

@article{jaffrayExistenceContinuousUtility1975,
  title = {Existence of a {{Continuous Utility Function}}: {{An Elementary Proof}}},
  shorttitle = {Existence of a {{Continuous Utility Function}}},
  author = {Jaffray, Jean-Yves},
  year = {1975},
  month = sep,
  journal = {Econometrica},
  volume = {43},
  number = {5/6},
  eprint = {1911340},
  eprinttype = {jstor},
  pages = {981},
  issn = {00129682},
  doi = {10.2307/1911340},
  urldate = {2024-01-14}
}

@article{pelegUtilityFunctionsPartially1970,
  title = {Utility {{Functions}} for {{Partially Ordered Topological Spaces}}},
  author = {Peleg, Bezalel},
  year = {1970},
  month = jan,
  journal = {Econometrica},
  volume = {38},
  number = {1},
  eprint = {1909243},
  eprinttype = {jstor},
  pages = {93},
  issn = {00129682},
  doi = {10.2307/1909243},
  urldate = {2024-01-14}
}

@article{voorneveldElementaryProofThat2016,
  title = {An {{Elementary Proof That Well-Behaved Utility Functions Exist}}},
  author = {Voorneveld, Mark and Weibull, J{\"o}rgen W.},
  year = {2016},
  journal = {Theoretical Economics Letters},
  volume = {06},
  number = {03},
  pages = {450--457},
  issn = {2162-2078, 2162-2086},
  doi = {10.4236/tel.2016.63051},
  urldate = {2024-01-14}
}

@article{yunAreTransformersUniversal,
  title = {Are {{Transformers}} Universal Approximators of Sequence-to-Sequence Functions?},
  author = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  abstract = {Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to selfattention layers and empirically evaluate them.},
  langid = {english}
}

@article{bachBreakingCurseDimensionality2016,
  title = {Breaking the {{Curse}} of {{Dimensionality}} with {{Convex Neural Networks}}},
  author = {Bach, Francis},
  year = {2016},
  month = oct,
  number = {arXiv:1412.8690},
  eprint = {1412.8690},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2024-02-04},
  archiveprefix = {arxiv}
}

@misc{altabaaRelationalConvolutionalNetworks2023,
  title={Learning Hierarchical Relational Representations through Relational Convolutions}, 
  author={Awni Altabaa and John Lafferty},
  year={2024},
  eprint={2310.03240},
  archivePrefix={arXiv},
  note={arXiv:2310.03240},
  primaryClass={cs.LG}
}

@misc{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and {Castro-Ros}, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = dec,
  note = {arXiv:2210.\-11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-07},
  archiveprefix = {arxiv}
}

@article{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-07},
  archiveprefix = {arxiv}
}

@article{taoriAlpacaStrongReplicable2023,
  title = {Alpaca: {{A}} Strong, Replicable Instruction-Following Model},
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2023},
  note = {Stanford Center for Research on Foundation Models},
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  note = {arXiv:2307.\-09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-12-06},
  archiveprefix = {arxiv}
}

@article{devlinBertPretrainingDeep2018,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  eprint = {1810.04805},
  archiveprefix = {arxiv}
}

@inproceedings{dongSpeechtransformerNorecurrenceSequencetosequence2018,
  title = {Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition},
  booktitle = {2018 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Dong, Linhao and Xu, Shuang and Xu, Bo},
  year = {2018},
  pages = {5884--5888},
  publisher = {{IEEE}},
  isbn = {1-5386-4658-7}
}

@article{dosovitskiyImageWorth16x162020,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
  year = {2020},
  eprint = {2010.11929},
  archiveprefix = {arxiv}
}

@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  pages = {10012--10022}
}

@article{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  journal = {The Journal of Machine Learning Research},
  volume = {21},
  number = {1},
  pages = {5485--5551},
  publisher = {{JMLRORG}},
  isbn = {1532-4435}
}

@article{gravesHybridComputingUsing2016a,
  title = {Hybrid Computing Using a Neural Network with Dynamic External Memory},
  author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and {Grabska-Barwi{\'n}ska}, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John},
  year = {2016},
  journal = {Nature},
  volume = {538},
  number = {7626},
  pages = {471--476},
  publisher = {{Nature Publishing Group UK London}},
  isbn = {0028-0836}
}

@misc{gravesNeuralTuringMachines2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = dec,
  number = {arXiv:1410.5401},
  eprint = {1410.5401},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1410.5401},
  urldate = {2023-06-11},
  archiveprefix = {arxiv}
}

@misc{locatelloObjectCentricLearningSlot2020b,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  month = oct,
  number = {arXiv:2006.15055},
  eprint = {2006.15055},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.15055},
  urldate = {2023-09-22},
  archiveprefix = {arxiv}
}

@inproceedings{santoroRelationalRecurrentNeural2018,
  title = {Relational Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-11-11}
}

@article{velickovicGraphAttentionNetworks2017a,
  title = {Graph Attention Networks},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  year = {2017},
  journal = {arXiv preprint arXiv:1710.10903},
  eprint = {1710.10903},
  archiveprefix = {arxiv}
}

@inproceedings{zambaldiDeepReinforcementLearning2018a,
  title = {Deep Reinforcement Learning with Relational Inductive Biases},
  booktitle = {International Conference on Learning Representations},
  author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward},
  year = {2018}
}

@inproceedings{pritzelNeuralEpisodicControl2017,
  title = {Neural Episodic Control},
  booktitle = {International Conference on Machine Learning},
  author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Badia, Adria Puigdomenech and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  year = {2017},
  pages = {2827--2836},
  publisher = {{PMLR}},
  isbn = {2640-3498}
}

@article{baldiNeuralNetworksFingerprint1993,
  title = {Neural Networks for Fingerprint Recognition},
  author = {Baldi, Pierre and Chauvin, Yves},
  year = {1993},
  journal = {neural computation},
  volume = {5},
  number = {3},
  pages = {402--418},
  publisher = {{MIT Press}},
  isbn = {0899-7667}
}

@article{bromleySignatureVerificationUsing1993,
  title = {Signature Verification Using a" Siamese" Time Delay Neural Network},
  author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S{\"a}ckinger, Eduard and Shah, Roopak},
  year = {1993},
  journal = {Advances in neural information processing systems},
  volume = {6}
}

@inproceedings{kochSiameseNeuralNetworks2015,
  title = {Siamese Neural Networks for One-Shot Image Recognition},
  booktitle = {{{ICML}} Deep Learning Workshop},
  author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
  year = {2015},
  volume = {2},
  publisher = {{Lille}}
}

@article{langTimedelayNeuralNetwork1988,
  title = {A Time-Delay Neural Network Architecture for Speech Recognition},
  author = {Lang, Kevin J.},
  year = {1988},
  journal = {Technical Report},
  publisher = {{Carnegie-Mellon University}}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  journal = {nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group UK London}},
  isbn = {0028-0836}
}
@misc{wright2021transformers,
      title={Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines}, 
      author={Matthew A. Wright and Joseph E. Gonzalez},
      year={2021},
      eprint={2106.01506},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{petrushev1998approximation,
  author={P.~P. Petrushev},
  title={Approximation by ridge functions and neural networks},
  journal={SIAM Journal on Mathematical Analysis},
  volume={30},
  number={1},
  pages={155--189},
  year={1998},
}

@article{pinkus1999approximation,
  author={A.~Pinkus},
  title={Approximation theory of the {MLP} model in neural networks},
  journal={Acta Numerica},
  volume={8},
  pages={143--195},
  year={1999},
}

@article{maiorov2006approximation,
  author={V.~Maiorov},
  title={Approximation by neural networks and learning theory},
  journal={Journal of Complexity},
  volume={22},
  number={1},
  pages={102--117},
  year={2006},
}

@article{burger2001error,
  author={M.~Burger and A.~Neubauer},
  title={Error bounds for approximation with neural networks},
  journal={Journal of Approximation Theory}, 
  volume={112},
  number={2},
  pages={235--250},
  year={2001},
}

@article{makovoz1998uniform,
  author={Y.~Makovoz},
  title={Uniform approximation by neural networks},
  journal={Journal of Approximation Theory}, 
  volume={95},
  number={2},
  pages={215--228},
  year={1998},
}

@article{wuExplicitNeuralNetwork2018,
  title = {An Explicit Neural Network Construction for Piecewise Constant Function Approximation},
  author = {Wu, Kailiang and Xiu, Dongbin},
  year = {2018},
  journal = {arXiv preprint arXiv:1808.07390},
  eprint = {1808.07390},
  archiveprefix = {arxiv}
}

@misc{svete2024transformers,
      title={Transformers Can Represent $n$-gram Language Models}, 
      author={Anej Svete and Ryan Cotterell},
      year={2024},
      eprint={2404.14994},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wyner1965capabilities,
  title={Capabilities of bounded discrepancy decoding},
  author={Wyner, Aaron D},
  journal={Bell System Technical Journal},
  volume={44},
  number={6},
  pages={1061--1122},
  year={1965},
  publisher={Wiley Online Library}
}

@article{maiorov1999lower,
  title={Lower bounds for approximation by MLP neural networks},
  author={Maiorov, Vitaly and Pinkus, Allan},
  journal={Neurocomputing},
  volume={25},
  number={1-3},
  pages={81--91},
  year={1999},
  publisher={Elsevier}
}

@article{devore1998nonlinear,
  title={Nonlinear approximation},
  author={DeVore, Ronald A},
  journal={Acta numerica},
  volume={7},
  pages={51--150},
  year={1998},
  publisher={Cambridge University Press}
}

@article{maiorov2000near,
  title={On the near optimality of the stochastic approximation of smooth functions by neural networks},
  author={Maiorov, VE and Meir, Ron},
  journal={Advances in Computational Mathematics},
  volume={13},
  pages={79--103},
  year={2000},
  publisher={Springer}
}

@misc{altabaa2024disentangling,
  title={Disentangling and Integrating Relational and Sensory Information in Transformer Architectures}, 
  author={Awni Altabaa and John Lafferty},
  year={2024},
  eprint={2405.16727},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  note={arXiv:2405.16727},
}
