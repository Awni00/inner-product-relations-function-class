@online{altabaaAbstractorsTransformer2023,
  title = {Abstractors: {{Transformer Modules}} for {{Symbolic Message Passing}} and {{Relational Reasoning}}},
  shorttitle = {Abstractors},
  author = {Altabaa, Awni and Webb, Taylor and Cohen, Jonathan and Lafferty, John},
  date = {2023-04-01},
  url = {https://arxiv.org/abs/2304.00195v2},
  urldate = {2023-06-28},
  langid = {english},
  pubstate = {preprint}
}

@article{barronUniversalApproximation1993,
  title = {Universal Approximation Bounds for Superpositions of a Sigmoidal Function},
  author = {Barron, A.R.},
  date = {1993-05},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {39},
  number = {3},
  pages = {930--945},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.256500},
  url = {https://ieeexplore.ieee.org/document/256500/},
  urldate = {2023-03-31}
}

@article{cybenkoApproximationSuperpositions1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  date = {1989-12},
  journaltitle = {Mathematics of Control, Signals, and Systems},
  shortjournal = {Math. Control Signal Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {0932-4194, 1435-568X},
  doi = {10.1007/BF02551274},
  url = {http://link.springer.com/10.1007/BF02551274},
  urldate = {2023-03-31},
  langid = {english}
}

@online{devitoExtensionMercer2011,
  title = {An Extension of {{Mercer}} Theorem to Vector-Valued Measurable Kernels},
  author = {De Vito, Ernesto and Umanita`, Veronica and Villa, Silvia},
  date = {2011-10-18},
  eprint = {1110.4017},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1110.4017},
  urldate = {2023-05-06},
  pubstate = {preprint}
}

@article{hornikMultilayerFeedforward1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
  urldate = {2022-10-07},
  langid = {english}
}

@online{kergNeuralArchitecture2022,
  title = {On {{Neural Architecture Inductive Biases}} for {{Relational Tasks}}},
  author = {Kerg, Giancarlo and Mittal, Sarthak and Rolnick, David and Bengio, Yoshua and Richards, Blake and Lajoie, Guillaume},
  date = {2022-06-09},
  eprint = {2206.05056},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.05056},
  url = {http://arxiv.org/abs/2206.05056},
  urldate = {2022-09-13},
  pubstate = {preprint}
}

@article{mercerFunctionsPositive1909,
  title = {Functions of {{Positive}} and {{Negative Type}}, and Their {{Connection}} with the {{Theory}} of {{Integral Equations}}},
  author = {Mercer, J.},
  date = {1909},
  journaltitle = {Philosophical Transactions of the Royal Society of London. Series A},
  volume = {209},
  eprint = {91043},
  eprinttype = {jstor},
  pages = {415--446},
  publisher = {{The Royal Society}},
  issn = {0264-3952},
  url = {https://www.jstor.org/stable/91043},
  urldate = {2023-03-31}
}

@article{micchelliUniversalKernels2006,
  title = {Universal {{Kernels}}},
  author = {Micchelli, Charles A. and Xu, Yuesheng and Zhang, Haizhang},
  date = {2006},
  journaltitle = {Journal of Machine Learning Research},
  volume = {7},
  number = {95},
  pages = {2651--2667},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v7/micchelli06a.html},
  urldate = {2023-03-31}
}

@inproceedings{santoroRelationalRecurrent2018,
  title = {Relational Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/e2eabaf96372e20a9e3d4b5f83723a61-Abstract.html},
  urldate = {2022-11-11}
}

@online{santoroSimpleNeural2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  date = {2017-06-05},
  eprint = {1706.01427},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.01427},
  urldate = {2022-11-11},
  pubstate = {preprint}
}

@article{seelyNonSymmetricKernels1919,
  title = {Non-{{Symmetric Kernels}} of {{Positive Type}}},
  author = {Seely, Dr. Caroline E.},
  date = {1919-03},
  journaltitle = {The Annals of Mathematics},
  shortjournal = {The Annals of Mathematics},
  volume = {20},
  number = {3},
  eprint = {1967866},
  eprinttype = {jstor},
  pages = {172--176},
  issn = {0003486X},
  doi = {10.2307/1967866},
  url = {https://www.jstor.org/stable/1967866?origin=crossref},
  urldate = {2023-04-13}
}

@article{sunMercerTheorem2005,
  title = {Mercer Theorem for {{RKHS}} on Noncompact Sets},
  author = {Sun, Hongwei},
  date = {2005-06},
  journaltitle = {Journal of Complexity},
  shortjournal = {Journal of Complexity},
  volume = {21},
  number = {3},
  pages = {337--349},
  issn = {0885064X},
  doi = {10.1016/j.jco.2004.09.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885064X04000822},
  urldate = {2023-03-31},
  langid = {english}
}

@article{aronszajn1950theory,
  title={Theory of reproducing kernels},
  author={Aronszajn, Nachman},
  journal={Transactions of the American mathematical society},
  volume={68},
  number={3},
  pages={337--404},
  year={1950}
}

@online{webbEmergentSymbols2021,
  title = {Emergent {{Symbols}} through {{Binding}} in {{External Memory}}},
  author = {Webb, Taylor W. and Sinha, Ishan and Cohen, Jonathan D.},
  date = {2021-03-09},
  eprint = {2012.14601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.14601},
  url = {http://arxiv.org/abs/2012.14601},
  urldate = {2022-09-13},
  pubstate = {preprint}
}

@inproceedings{zhangReproducingKernel2009,
  title = {Reproducing Kernel {{Banach}} Spaces for Machine Learning},
  booktitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Zhang, Haizhang and Xu, Yuesheng and Zhang, Jun},
  date = {2009-06},
  pages = {3520--3527},
  publisher = {{IEEE}},
  location = {{Atlanta, Ga, USA}},
  doi = {10.1109/IJCNN.2009.5179093},
  url = {http://ieeexplore.ieee.org/document/5179093/},
  urldate = {2023-05-06},
  eventtitle = {2009 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}} 2009 - {{Atlanta}})},
  isbn = {978-1-4244-3548-7},
  langid = {english}
}

@inproceedings{chopraLearningSimilarityMetric2005,
  author={Chopra, S. and Hadsell, R. and LeCun, Y.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={Learning a similarity metric discriminatively, with application to face verification}, 
  year={2005},
  pages={539-546},
  doi={10.1109/CVPR.2005.202}}

@article{seelyNonSymmetricKernelsPositive1919,
  title = {Non-{{Symmetric Kernels}} of {{Positive Type}}},
  author = {Seely, Dr. Caroline E.},
  year = {1919},
  month = mar,
  journal = {The Annals of Mathematics},
  volume = {20},
  number = {3},
  eprint = {1967866},
  eprinttype = {jstor},
  pages = {172},
  issn = {0003486X},
  doi = {10.2307/1967866},
  urldate = {2023-04-13}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{mhaskarDeepVsShallow2016,
  title = {Deep vs. Shallow Networks : {{An}} Approximation Theory Perspective},
  shorttitle = {Deep vs. Shallow Networks},
  author = {Mhaskar, Hrushikesh and Poggio, Tomaso},
  year = {2016},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-01-03},
  abstract = {The paper briefy reviews several recent results on hierarchical architectures for learning from examples, that may formally explain the conditions under which Deep Convolutional Neural Networks perform much better in function approximation problems than shallow, one-hidden layer architectures. The paper announces new results for a non-smooth activation function - the ReLU function - used in present-day neural networks, as well as for the Gaussian networks. We propose a new definition of relative dimension to encapsulate different notions of sparsity of a function class that can possibly be exploited by deep networks but not by shallow ones to drastically reduce the complexity required for approximation and learning.},
  howpublished = {https://arxiv.org/abs/1608.03287v1},
  langid = {english}
}

@misc{okunoProbabilisticFrameworkMultiview2018,
  title = {A Probabilistic Framework for Multi-View Feature Learning with Many-to-Many Associations via Neural Networks},
  author = {Okuno, Akifumi and Hada, Tetsuya and Shimodaira, Hidetoshi},
  year = {2018},
  month = jun,
  number = {arXiv:1802.04630},
  eprint = {1802.04630},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2024-01-01},
  abstract = {A simple framework Probabilistic Multi-view Graph Embedding (PMvGE) is proposed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a probabilistic model for predicting new associations via graph embedding of the nodes of data vectors with links of their associations. Multi-view data vectors with many-to-many associations are transformed by neural networks to feature vectors in a shared space, and the probability of new association between two data vectors is modeled by the inner product of their feature vectors. While existing multi-view feature learning techniques can treat only either of many-to-many association or non-linear transformation, PMvGE can treat both simultaneously. By combining Mercer's theorem and the universal approximation theorem, we prove that PMvGE learns a wide class of similarity measures across views. Our likelihood-based estimator enables efficient computation of non-linear transformations of data vectors in large-scale datasets by minibatch SGD, and numerical experiments illustrate that PMvGE outperforms existing multi-view methods.},
  archiveprefix = {arxiv}
}

@article{poggioWhyWhenCan2017,
  title = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality: {{A}} Review},
  shorttitle = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  year = {2017},
  month = oct,
  journal = {International Journal of Automation and Computing},
  volume = {14},
  number = {5},
  pages = {503--519},
  issn = {1476-8186, 1751-8520},
  doi = {10.1007/s11633-017-1054-2},
  urldate = {2024-01-03},
  abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
  langid = {english}
}

@article{debreuRepresentationPreferenceOrdering1954,
  title = {Representation of a Preference Ordering by a Numerical Function},
  author = {Debreu, Gerard},
  year = {1954},
  journal = {Decision processes},
  volume = {3},
  pages = {159--165},
  publisher = {{Wiley New York}}
}

@article{jaffrayExistenceContinuousUtility1975,
  title = {Existence of a {{Continuous Utility Function}}: {{An Elementary Proof}}},
  shorttitle = {Existence of a {{Continuous Utility Function}}},
  author = {Jaffray, Jean-Yves},
  year = {1975},
  month = sep,
  journal = {Econometrica},
  volume = {43},
  number = {5/6},
  eprint = {1911340},
  eprinttype = {jstor},
  pages = {981},
  issn = {00129682},
  doi = {10.2307/1911340},
  urldate = {2024-01-14}
}

@article{pelegUtilityFunctionsPartially1970,
  title = {Utility {{Functions}} for {{Partially Ordered Topological Spaces}}},
  author = {Peleg, Bezalel},
  year = {1970},
  month = jan,
  journal = {Econometrica},
  volume = {38},
  number = {1},
  eprint = {1909243},
  eprinttype = {jstor},
  pages = {93},
  issn = {00129682},
  doi = {10.2307/1909243},
  urldate = {2024-01-14}
}

@article{voorneveldElementaryProofThat2016,
  title = {An {{Elementary Proof That Well-Behaved Utility Functions Exist}}},
  author = {Voorneveld, Mark and Weibull, J{\"o}rgen W.},
  year = {2016},
  journal = {Theoretical Economics Letters},
  volume = {06},
  number = {03},
  pages = {450--457},
  issn = {2162-2078, 2162-2086},
  doi = {10.4236/tel.2016.63051},
  urldate = {2024-01-14}
}

@article{yunAreTransformersUniversal,
  title = {Are {{Transformers}} Universal Approximators of Sequence-to-Sequence Functions?},
  author = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  abstract = {Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to selfattention layers and empirically evaluate them.},
  langid = {english}
}

@misc{bachBreakingCurseDimensionality2016,
  title = {Breaking the {{Curse}} of {{Dimensionality}} with {{Convex Neural Networks}}},
  author = {Bach, Francis},
  year = {2016},
  month = oct,
  number = {arXiv:1412.8690},
  eprint = {1412.8690},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2024-02-04},
  archiveprefix = {arxiv}
}

@misc{altabaaRelationalConvolutionalNetworks2023,
  title = {Relational Convolutional Networks: A Framework for Learning Representations of Hierarchical Relations},
  shorttitle = {Relational Convolutional Networks},
  author = {Altabaa, Awni and Lafferty, John},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03240},
  eprint = {2310.03240},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-30},
  archiveprefix = {arxiv}
}

@misc{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and {Castro-Ros}, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = dec,
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-07},
  archiveprefix = {arxiv}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  month = dec,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-02-07},
  archiveprefix = {arxiv}
}

@article{taoriAlpacaStrongReplicable2023,
  title = {Alpaca: {{A}} Strong, Replicable Instruction-Following Model},
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2023},
  journal = {Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume = {3},
  number = {6},
  pages = {7}
}

@misc{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.09288},
  urldate = {2023-12-06},
  archiveprefix = {arxiv}
}

@article{devlinBertPretrainingDeep2018,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.04805},
  eprint = {1810.04805},
  archiveprefix = {arxiv}
}

@inproceedings{dongSpeechtransformerNorecurrenceSequencetosequence2018,
  title = {Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition},
  booktitle = {2018 {{IEEE}} International Conference on Acoustics, Speech and Signal Processing ({{ICASSP}})},
  author = {Dong, Linhao and Xu, Shuang and Xu, Bo},
  year = {2018},
  pages = {5884--5888},
  publisher = {{IEEE}},
  isbn = {1-5386-4658-7}
}

@article{dosovitskiyImageWorth16x162020,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain},
  year = {2020},
  journal = {arXiv preprint arXiv:2010.11929},
  eprint = {2010.11929},
  archiveprefix = {arxiv}
}

@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin Transformer: {{Hierarchical}} Vision Transformer Using Shifted Windows},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} International Conference on Computer Vision},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  pages = {10012--10022}
}

@article{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  journal = {The Journal of Machine Learning Research},
  volume = {21},
  number = {1},
  pages = {5485--5551},
  publisher = {{JMLRORG}},
  isbn = {1532-4435}
}
